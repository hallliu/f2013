\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{1}
\ssn{a}
Let $A$ be a $n\times 1$ matrix over $\cn$. Its matrix $2$-norm is $\sup_{|z|=1}\|Az\|_2$, where $z$ and $Az$ are scalars/$1\times 1$ vectors. If $a_i$ are the entries of $A$, then $\|Az\|=\openm a_1z&\cdots&a_nz\closem^T$, so its $2$-norm is $|z|\|A\|_2$, where the norm of $A$ is taken as a vector norm. However, since $|z|=1$, this is just $\|A\|_2$, the vector norm.

In the case where $A$ is a $1\times n$ matrix, we have $Ax=\langle A^T,x\rangle$. The $2$-norm of this is just the complex modulus, so by the Cauchy-Schwarz inequality, we have $|Ax|^2\leq\langle A^T,A^T\rangle\cdot\langle x,x\rangle$. If we're calculating the matrix norm, we can set $\|x\|_2^2$ to $1$, so $|Ax|^2\leq\langle A^T,A^T\rangle=\|A\|_2^2$, where the norm on the LHS is the vector norm of $A$. Since the Cauchy-Schwarz inequality provides for equality for certain $x$, we must have that the matrix norm of $A$ is equal to the vector norm of $A$ as well.
\ssn{b}
Label the columns of $U$ as $u_1,u_2,\ldots,u_n$. Then $Ux=\sum u_ix_i$, where $x_i$ is a scalar. Then, since the $u_i$ are orthogonal to each other, the Pythagoran theorem gives $\|Ux\|_2^2=\sum \|u_i\|_2^2|x_i|^2=\sum|x_i|^2=\|x\|_2^2$.
\ssn{c}
Consider the matrix $U=\frac{1}{\sqrt{2}}\openm1&1\\1&-1\closem$. This is unitary because it's symmetric and it's its own inverse. Applied to the first standard basis vector, it results in $Ue_1=\openm2^{-1/2}\\2^{-1/2}\closem$. We then have $\|Ue_1\|_p=\left(2\cdot2^{-p/2}\right)^{1/p}$. If the $p$-norm is to be unitarily invariant, this expression must equal $1$, so we need $2\cdot2^{-p/2}=1\implies2^{-p/2}=2^{-1}\implies p=2$. Thus, if $p\neq2$, the $p$-norm is not unitarily invariant.
\ssn{d}
Consider $\|UAVx\|_2$ for $\|x\|_2=1$. This is the same as considering $\|UAx\|_2$ for $\|x\|_2=1$, since $V$ preserves norms and is invertible. However, we also have that $\|U(Ax)\|_2=\|Ax\|_2$ for all $x$ because $U$ is also unitary, so the set of complex values $\|UAVx\|_2$ is the same as the set of complex values $\|Ax\|_2$ for $\|x\|_2=1$, so their sups must be the same, and therefore the matrix norm of $UAV$ is the same as the matrix norm of $A$.
\ssn{e}
The $ii$th entry of $A^*A$ is $\sum_j|a_{ji}|^2$, and same for the $ii$th entry of $AA^*$. Summing over $i$ in both cases gives a sum of squares of every entry in the matrix, which is the same as $\|A\|_F$. 

Then, $\|UAV\|_F=\tr(UAVV^*A^*U^*)=\tr(UAA^*U^*)$. Since $U^*=U^{-1}$, $UAA^*U^*$ is in the same conjugacy class as $AA^*$, so they have the same trace. Thus, this is the same as the trace of $AA^*$, which is in turn the Frobenius norm of $A$.
\ssn{f}
(i)$\implies$(iii):
Consider the action of $U$ on $e_i+e_j$ where the $e_i$ are the standard basis vectors and $i\neq j$. Since $U$ preserves the $2$-norm, we have that $\|Ue_i+Ue_j\|^2=2$, or $\langle Ue_i+Ue_j,Ue_i+Ue_j\rangle=2$. Expanding this, we get 
\begin{align*}
    &\langle Ue_i,Ue_i\rangle+\langle Ue_j,Ue_j\rangle+\langle Ue_i,Ue_j\rangle+\conj{\langle Ue_i,Ue_j\rangle}=2\\
    &\implies2+2\text{Re}(\langle Ue_i,Ue_j\rangle)=2\\
    &\implies \text{Re}(\langle Ue_i,Ue_j\rangle)=0
\end{align*}

Repeating this calculation for $e_i+ie_j$ gives that $\text{Im}(\langle Ue_i,Ue_j\rangle)=0$, so the columns of $U$ must be orthogonal to each other. In addition, they must be orthonormal as well because $\inner{Ue_i}{Ue_i}=1$, and from this we have immediately that $U$ is unitary.

(iii)$\implies$(ii): $\inner{Ux}{Uy}=\sum_{i,j}\conj{x_i}y_j\inner{u_i}{u_j}=\sum_i\conj{x_i}y_i=\inner{x}{y}$ since the columns of $U$ are orthonormal.

(ii)$\implies$(i): $\|Ux\|_2^2=\inner{Ux}{Ux}=\inner{x}{x}=\|x\|_2^2$ by (ii).
\subsection*{2}
Examine $\|(I-A)v\|_\alpha=\|v-Av\|_\alpha$. Restrict our consideration to $v$ with norm $1$ (since if $A$ were singular, it'd have to send a whole subspace to $0$, and any nonzero subspace contains some vector with norm $1$). Then, by the triangle inequality, $\|v-Av\|_\alpha\geq\left|1-\|Av\|_\alpha\right|>0$, since $\|Av\|_\alpha$ is less than $1$ because $\|A\|_\alpha<1$. Thus by positive-definiteness, $(I-A)v\neq0$ if $v\neq0$.

By the sub-multiplicativity of the induced norm, we have $\|I-A\|\|(I-A)^{-1}\|\geq1$, and by the triangle inequality we have $\|I-A\|\leq1+\|A\|$, so combining these inequalities gives $\|(I-A)^{-1}\|\geq\frac{1}{1+\|A\|}$. For the other side of the inequality, write $(I-A)^{-1}=I+\sum_{i=1}^\infty A^i$ by analogy with the standard series expansion for $\frac{1}{1-x}$. This converges since $\|A\|<1$, and the difference between arbitrary partial sums is less than the difference between the corresponding partial sum of the geometric series of $\|A\|$. Then, we can take the norm of both sides, obtaining $\|(I-A)^{-1}\|\leq\sum_{i=0}^\infty\|A\|^i=\frac{1}{1-\|A\|}$.
\subsection*{3}
Relevant code is attached at the end. Everything is in Python, written to be Python3 compatible.

Here are tables recording the relevant values for $n=5$.

\vspace{10pt}
\begin{tabular}{c|c|c|c}
    normal&1-norm&2-norm&inf-norm\\
    \hline
        $\frac{\|\h{x}-x\|}{\|x\|}$&1.998e-16&2.275e-16&3.331e-16\\
        $\kappa(A)$&1.471e+01&5.309e+00&8.624e+00\\
        $\kappa(A)\frac{\|\delta b\|}{\|b\|}$&5.741e-15&2.134e-15&3.588e-15\\
lb\end{tabular}


\vspace{10pt}
\begin{tabular}{c|c|c|c}
        hilbert&1-norm&2-norm&inf-norm\\
        \hline
            $\frac{\|\h{x}-x\|}{\|x\|}$&2.552e-13&3.493e-13&6.168e-13\\
            $\kappa(A)$&9.437e+05&4.766e+05&9.437e+05\\
            $\kappa(A)\frac{\|\delta b\|}{\|b\|}$&1.623e-11&1.686e-11&4.588e-11\\
\end{tabular}


\vspace{10pt}
\begin{tabular}{c|c|c|c}
        pascal&1-norm&2-norm&inf-norm\\
        \hline
            $\frac{\|\h{x}-x\|}{\|x\|}$&0.000e+00&0.000e+00&0.000e+00\\
            $\kappa(A)$&1.562e+04&8.518e+03&1.562e+04\\
            $\kappa(A)\frac{\|\delta b\|}{\|b\|}$&0.000e+00&0.000e+00&0.000e+00\\
\end{tabular}


\vspace{10pt}
\begin{tabular}{c|c|c|c}
        magic&1-norm&2-norm&inf-norm\\
        \hline
            $\frac{\|\h{x}-x\|}{\|x\|}$&1.554e-16&1.790e-16&2.220e-16\\
            $\kappa(A)$&6.850e+00&5.462e+00&6.850e+00\\
            $\kappa(A)\frac{\|\delta b\|}{\|b\|}$&2.995e-16&5.340e-16&1.498e-15\\
\end{tabular}

The graph for the other dimensions follows. The graphs are ordered Normal, Hilbert, Pascal, Magic going down the rows, and $\frac{\|\h{x}-x\|}{\|x\|}$, $\kappa(A)$, and $\kappa(A)\frac{\|\delta b\|}{\|b\|}$ going across the columns. 

\includegraphics[width=0.95\textwidth]{hw1_files/graph.png}

On all the graphs, red indicates the $1$-norm, blue indicates the $2$-norm, and green indicates the $\infty$-norm.

\ssn{d}
Due to the large spread in the data for all the matrices, it was impossible to get meaningful data on anything but a log-log plot. For the random normal matrices in particular, it seems that relative error in $x$ increases linearly with dimension, which makes sense because the condition number increases linearly and the relative error in $x$ is bounded by the condition number times the relative error in $b$. Since $b$ is calculated fairly simply, we'd expect its relative error to be fairly constant. 

As for the trend in the norms, there is a consistent trend $\|\cdot\|_1<\|\cdot\|_2<\|\cdot\|_\infty$. In fact, this trend in the norms seems to hold true for all the matrix types. This can be explained by $\delta x$ being usually fairly small, where the norms don't differ much, and the denominator changing rapidly from $n$ to $\sqrt{n}$ to $1$ as we move down the norms. 

For the Hilbert matrices, the trend seems to be logarithmic. This can be explained by looking at the infinity norm of $A$ -- the maximum row sum is the sum along the first row, which is just the harmonic series as $n$ increases, and this is known to increase logarithmically.

For the Pascal matrices, the trend is strikingly exponential, with a simple log-plot showing a straight line. The largest entry is always a binomial coeffient of the form $\binom{2k}{k}$, which is asymptotically exponential. It then follows that the max row/column sum would be bounded above and below by some factor that is asymptotically dominated by this exponential factor, so the condition number would be asymptotically exponential as well.

Finally, the magic square matrices show an interesting trend. The matrices with odd dimension have low errors/condition numbers, whereas the matrices with even dimension have high errors/condition numbers. Both increase roughly linearly with dimension, though at different scales. Presumably this has something to do with the construction of magic squares, for which methods differ for odd, doubly even, and even dimensions, but I'm not familiar enough with these to comment on them.
\ssn{e}
Taking the inverse incurs a huge hit to accuracy. Below is a graph presenting the ratio of $\frac{\|x-\h{x}\|}{\|x\|}$ in the inverse case to that in the solve case, arranged as Normal, Hilbert, Pascal, Magic from left to right, top to bottom.

\includegraphics[width=0.7\textwidth]{hw1_files/inverr.png}

In addition, the median of these ratios across all dimensions is as follows:

\begin{tabular}{c|c}
    Normal&2.5\\
    Hilbert&$1.12\times10^{11}$\\
    Pascal&$2.02\times10^{12}$\\
    Magic&7.018\\
\end{tabular}

\ssn{f}
By Cramer's rule, the $(1,1)$ entry of $A^{-1}$ is just $\frac{1}{\det(A)}C_{11}$, where $C_{11}$ is the determinant of $A$ with its first row and column removed. The function to do this is in the attached program as \verb|inv11| and \verb|inv11_test|.

\subsection*{4}
\ssn{a}
Suppose $(A-\delta A)$ is singular. Then there exists some $v$ of norm $1$ in its kernel. Examine $\frac{1}{\kappa(A)}$. This is equal to
\begin{align*}
    &\frac{1}{\|A\|\|A^{-1}\|}\\
    =&\frac{1}{\|A\|\max\frac{\|A^{-1}x\|}{\|x\|}}\\
    \leq&\frac{1}{\|A\|\frac{\|A^{-1}Av\|}{\|Av\|}}\\
    =&\frac{\|Av\|}{\|A\|\|v\|}\\
    =&\frac{\|(A-(A+\delta A))v\|}{\|A\|\|v\|}\\
    =&\frac{\|\delta Av\|}{\|A\|\|v\|}\\
    \leq&\frac{\|\delta A\|\|v\|}{\|A\|\|v\|}
\end{align*}
This contradicts the condition that we were given, so no such $v$ may exist.
\ssn{b}
Expanding, we have $b=Ax+A\delta x+\delta Ax + \delta A\delta x=b$, or $A\delta x+\delta A\h{x}=0$, so $\delta x=-A^{-1}\delta A\h{x}$. Taking the norm on both sides and using compatibility gives $\|\delta x\|\leq\|A^{-1}\|\|\delta A\|\|\h{x}\|$, or $\frac{\|\delta x\|}{\|\h{x}\|}\leq\|A^{-1}\|\frac{\|A\|}{\|A\|}\|\delta A\|=\kappa(A)\frac{\|\delta A\|}{\|A\|}$.
\ssn{c}
Starting from the result in (b), multiplying both sides and applying the triangle inequality to $\|\h{x}\|$ gives 
\begin{align*}
    \|\delta x\|&\leq\kappa(A)\frac{\|\delta A\|}{\|A\|}(\|x\|+\|\delta x\|)\\
    \left(1-\kappa(A)\frac{\|\delta A\|}{\|A\|}\right)\|\delta x\|&\leq\kappa(A)\frac{\|\delta A\|}{\|A\|}\|x\|\\
\end{align*}
Since (4.2) guarantees that the LHS is nonzero, we can divide by $\left(1-\kappa(A)\frac{\|\delta A\|}{\|A\|}\right)$ and achieve our desired result.
\ssn{d}
Expanding gives $Ax+A\delta x+\delta Ax+\delta A\delta x = b+\delta b$, or $A\delta x+\delta A\h{x}=\delta b$ after subtracting $b$ on both sides. Moving the $\delta A$ term over and multiplying both sides by $A^{-1}$ gives $\delta x=A^{-1}(\delta b-\delta A\h{x})$.

Taking the norm on both sides and using compatibility, we have
\begin{align*}
    \|\delta x\|&\leq\|A^{-1}\|\|\delta b-\delta A\h{x}\|\\
    &\leq\kappa(A)\frac{1}{\|A\|}(\|\delta b\|+\|\delta A\|\|\h{x}\|)\\
    &\leq\kappa(A)\left(\frac{\|\delta b\|}{\|A\|} + \frac{\|\delta A\|\|\h{x}\|}{\|A\|}\right)\\
    \frac{\|\delta x\|}{\|\h{x}\|}&\leq\kappa(A)\left(\frac{\|\delta A\|}{\|A\|}+\frac{\|\delta b\|}{\|A\|\|\h{x}\|}\right)\\
\end{align*}

Now, note that $(A+\delta A)\h{x}=\h{b}$ implies that
\begin{align*}
    \|A+\delta A\|\|\h{x}\|&\geq\|\h{b}\|\\
    \|\h{x}\|&\geq\frac{\|\h{b}\|}{\|A+\delta A\|}\\
    \frac{\|A+\delta A\|}{\|\h{b}\|}&\geq\frac{1}{\|\h{x}\|}\\
    \frac{\|A\|+\|\delta A\|}{\|\h{b}\|}&\geq\frac{1}{\|\h{x}\|}
\end{align*}

Then, substituting this into the RHS of the first expression derived above gives
\begin{align*}
    \frac{\|\delta x\|}{\|\h{x}\|}&\leq\kappa(A)\left(\frac{\|\delta A\|}{\|A\|}+\frac{\|\delta b\|(\|A\|+\|\delta A\|)}{\|A\|\|\h{b}\|}\right)\\
    \frac{\|\delta x\|}{\|\h{x}\|}&\leq\kappa(A)\left(\frac{\|\delta A\|}{\|A\|}+\frac{\|\delta b\|}{\|\h{b}\|}+\frac{\|\delta b\|\|\delta A\|}{\|\h{b}\|\|A\|}\right)\\
\end{align*}
\ssn{e}
Begin with $\|\delta x\|\leq\kappa(A)\frac{1}{\|A\|}(\|\delta b\|+\|\delta A\|\|\h{x}\|)$ from the last problem and expand $\h{x}$. We then have
\begin{align*}
    \|\delta x\|&\leq\kappa(A)\left(\frac{\|\delta b\|}{\|A\|}+\frac{\|\delta A\|}{\|A\|}\|x\|+\frac{\|\delta A\|}{\|A\|}\|\delta x\|)\right)\\
    \|\delta x\|-\kappa(A)\frac{\|\delta A\|}{\|A\|}\|\delta x\|&\leq\kappa(A)\left(\frac{\|\delta b\|}{\|A\|}+\frac{\|\delta A\|}{\|A\|}\|x\|\right)\\
    \|\delta x\|&\leq\frac{\kappa(A)\left(\frac{\|\delta b\|}{\|A\|}+\frac{\|\delta A\|}{\|A\|}\|x\|\right)}{1-\kappa(A)\frac{\|\delta A\|}{\|A\|}}\\
    \frac{\|\delta x\|}{\|x\|}&\leq\frac{\kappa(A)\left(\frac{\|\delta b\|}{\|A\|\|x\|}+\frac{\|\delta A\|}{\|A\|}\right)}{1-\kappa(A)\frac{\|\delta A\|}{\|A\|}}
\end{align*}
Then, by compatibility, we can turn the $\|A\|\|x\|$ in the denominator in the numerator into $\|Ax\|=\|b\|$, which gives the result we want.

\subsection*{5}
\ssn{a}
Consider the function $f:\cn^{m\times n}\times\cn^{n\times p}\to\rn$ defined by $f(A,B)=\frac{\|AB\|}{\|A\|\|B\|}$. The range of this function is the same as if we just restricted $A$ and $B$ to have norm $1$, since the scalar multiples factor out of the norm. Thus, since all norms are topologically equivalent, we can view $f$ as a continuous function mapping the product of two unit spheres into $\rn$. Since the unit sphere is compact, the image of $f$ is also compact, which means it's bounded above by some $K\in\rn$. Then, if we let $c>K$, we have $\frac{\|AB\|_c}{\|A\|_c\|B\|_c}=\frac{1}{c}f(A,B)<1$, which means that $\|\cdot\|_c$ is submultiplicative.

For the H\"{o}lder $\infty$ norm, fix $\|\cdot\|=\|\cdot\|_{H,\infty}$. Let the largest element of $A$ be $a$ and the largest element of $B$ be $b$. Then, the largest element of $AB$ has the form $\sum_ka_{ik}b_{kj}\leq\sum_kab=n\|A\|\|B\|$, so we can let $c=n$.
\ssn{b}
We only have to verify it for one Jordan block, since $D_\ep$ can also be seen as a block matrix. Then, let $D_{\ep,r}$ be the diagonal block corresponding to the location of the $r$th Jordan block of size $n_r$. We have
\begin{align*}
    D_{\ep,r}^{-1}J_rD_{\ep,r}&=\openm\ep^{-a}&&\\&\ddots&\\&&\ep^{-(a+n_r)}\closem\openm\lambda_r&1&&\\&\ddots&\ddots&\\&&\ddots&1\\&&&\lambda_r\closem\openm\ep^{a}&&\\&\ddots&\\&&\ep^{(a+n_r)}\closem\\
                              &=\openm\ep^{-a}&&\\&\ddots&\\&&\ep^{-(a+n_r)}\closem\openm\ep^a\lambda_r&\ep^{a+1}&&\\&\ddots&\ddots&\\&&\ddots&\ep^{(a+n_r)}\\&&&\ep^{-(a+n_r)}\lambda_r\closem\\
                              &=\openm\lambda_r&\ep&&\\&\ddots&\ddots&\\&&\ddots&\ep\\&&&\lambda_r\closem
\end{align*}
\ssn{c}
The infinity operator norm is the maximum of the sum along the rows. Since $D_\ep^{-1}JD_\ep$ takes on the same form as the Jordan matrix, the sum along the rows is always either $\lambda_r+\ep$ or $\lambda_r$ for some eigenvalue $\lambda_r$. Since $\rho(J)$ is the largest eigenvalue of $J$, the row sum will never be greater than the largest eigenvalue plus $\ep$.
\ssn{d}
Let $A=P^{-1}JP$, and define $\|x\|=\|D_\ep^{-1}Px\|_\infty$ for $x\in\cn^n$. Then, $\frac{\|Ax\|}{\|x\|}=\frac{\|D_\ep^{-1}PAx\|_\infty}{\|D_\ep^{-1}Px\|_\infty}$. However, note that maximizing this over all $x\in\cn^n$ is equivalent to maximizing this over all $P^{-1}D_\ep x\in\cn^n$, since $D_\ep$ and $P$ are both bijective. Then, we have that the norm induced on $A$ by this vector norm is
\[\|A\|=\max\frac{\|D_\ep^{-1}PAP^{-1}D_\ep x\|_\infty}{\|x\|_\infty}=\|D_\ep^{-1}JD_\ep\|_\infty\]
Now, the maximum row-sum of $D_\ep^{-1}JD_\ep$ will always be at least as large as the largest eigenvalue of $J$, since there is always going to be one row containing solely the largest eigenvalue. Thus, since the eigenvalues of $A$ and $J$ are identical, we have our desired result following from (c).
\subsection*{6}
\ssn{a}
For $n=12$, $\|A\|_1=48$ and $\|A\|_\infty=78$. For $n=25$, the same values are $325$ and $181$, respectively.
\ssn{b}
For $n=12$, $\|A\|_2=47.74$ and $\rho(A)=32.23$. For $n=25$, the same values are $180.8$ and $77.98$, respectively.
\ssn{c}
From the description of the matrix we're given, the value of the $i$th diagonal element is simply $n-i+1$, where $n$ is the dimension of the matrix. For all rows but the first, the one value to the left of the diagonal should be the same as the diagonal, and the values to the right should fall off one by one as we move to the right, ending at $1$ at the very right column of the matrix. Their sum is simply the sum of all natural numbers less than or equal to the diagonal element. Thus, for each $i$ from $1$ to $n-1$, there is a Gershgorin disc centered at $i$ with radius $\frac{i(i-1)}{2}$, and an additional disc centered at $n$ with radius $\frac{(n-1)(n-2)}{2}$.
\ssn{d}
$n=12$:

\begin{tabular}{cc}
    Eigenvalues&Singular values\\
     \hline
         $32.2289$&$47.7360$\\
         $20.1990$&$18.0281$\\
         $12.3111$&$10.7908$\\
         $6.9615$&$7.6225$\\
         $3.5119$&$5.8980$\\
         $1.5540$&$4.8270$\\
         $0.6435$&$4.0001$\\
         $0.0310$&$3.2112$\\
         $0.0495$&$2.4240$\\
         $0.0812$&$1.6387$\\
         $0.1436$&$0.8694$\\
         $0.2847$&$0.0000$\\
\end{tabular}

$n=25$:

\begin{tabular}{cc}
    Eigenvalues&Singular values\\
     \hline
         $77.9837$&$180.7546$\\
         $60.5984$&$70.3443$\\
         $47.7777$&$43.2984$\\
         $37.5667$&$31.1583$\\
         $29.2021$&$24.3174$\\
         $22.2856$&$19.9658$\\
         $16.5772$&$16.9829$\\
         $11.9193$&$14.8370$\\
         $8.2006$&$13.2461$\\
         $5.3359$&$12.0457$\\
         $3.2479$&$11.1084$\\
         $1.8424$&$10.2940$\\
         $1.3040$&$9.5057$\\
         $1.0630+0.4912i$&$8.7194$\\
         $1.0630$&$7.9330$\\
         $0.6357+0.7764i$&$7.1464$\\
         $0.6357$&$6.3598$\\
         $0.1830+0.8436i$&$5.5729$\\
         $0.1830$&$4.7859$\\
         $-0.2062+0.7243i$&$3.9986$\\
         $-0.2062$&$3.2112$\\
         $-0.4788+0.4793i$&$2.4240$\\
         $-0.4788$&$1.6387$\\
         $-0.6175+0.1670i$&$0.8694$\\
         $-0.6175$&$0.0000$\\
 \end{tabular}
\end{document}
