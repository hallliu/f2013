\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{1}
\ssn{a}
Let $A$ be a $n\times 1$ matrix over $\cn$. Its matrix $2$-norm is $\sup_{|z|=1}\|Az\|_2$, where $z$ and $Az$ are scalars/$1\times 1$ vectors. If $a_i$ are the entries of $A$, then $\|Az\|=\openm a_1z&\cdots&a_nz\closem^T$, so its $2$-norm is $|z|\|A\|_2$, where the norm of $A$ is taken as a vector norm. However, since $|z|=1$, this is just $\|A\|_2$, the vector norm.

In the case where $A$ is a $1\times n$ matrix, we have $Ax=\langle A^T,x\rangle$. The $2$-norm of this is just the complex modulus, so by the Cauchy-Schwarz inequality, we have $|Ax|^2\leq\langle A^T,A^T\rangle\cdot\langle x,x\rangle$. If we're calculating the matrix norm, we can set $\|x\|_2^2$ to $1$, so $|Ax|^2\leq\langle A^T,A^T\rangle=\|A\|_2^2$, where the norm on the LHS is the vector norm of $A$. Since the Cauchy-Schwarz inequality provides for equality for certain $x$, we must have that the matrix norm of $A$ is equal to the vector norm of $A$ as well.
\ssn{b}
Label the columns of $U$ as $u_1,u_2,\ldots,u_n$. Then $Ux=\sum u_ix_i$, where $x_i$ is a scalar. Then, since the $u_i$ are orthogonal to each other, the Pythagoran theorem gives $\|Ux\|_2^2=\sum \|u_i\|_2^2|x_i|^2=\sum|x_i|^2=\|x\|_2^2$.
\ssn{c}
Consider the matrix $U=\frac{1}{\sqrt{2}}\openm1&1\\1&-1\closem$. This is unitary because it's symmetric and it's its own inverse. Applied to the first standard basis vector, it results in $Ue_1=\openm2^{-1/2}\\2^{-1/2}\closem$. We then have $\|Ue_1\|_p=\left(2\cdot2^{-p/2}\right)^{1/p}$. If the $p$-norm is to be unitarily invariant, this expression must equal $1$, so we need $2\cdot2^{-p/2}=1\implies2^{-p/2}=2^{-1}\implies p=2$. Thus, if $p\neq2$, the $p$-norm is not unitarily invariant.
\ssn{d}
Consider $\|UAVx\|_2$ for $\|x\|_2=1$. This is the same as considering $\|UAx\|_2$ for $\|x\|_2=1$, since $V$ preserves norms and is invertible. However, we also have that $\|U(Ax)\|_2=\|Ax\|_2$ for all $x$ because $U$ is also unitary, so the set of complex values $\|UAVx\|_2$ is the same as the set of complex values $\|Ax\|_2$ for $\|x\|_2=1$, so their sups must be the same, and therefore the matrix norm of $UAV$ is the same as the matrix norm of $A$.
\ssn{e}
The $ii$th entry of $A^*A$ is $\sum_j|a_{ji}|^2$, and same for the $ii$th entry of $AA^*$. Summing over $i$ in both cases gives a sum of squares of every entry in the matrix, which is the same as $\|A\|_F$. 

Then, $\|UAV\|_F=\tr(UAVV^*A^*U^*)=\tr(UAA^*U^*)$. Since $U^*=U^{-1}$, $UAA^*U^*$ is in the same conjugacy class as $AA^*$, so they have the same trace. Thus, this is the same as the trace of $AA^*$, which is in turn the Frobenius norm of $A$.
\ssn{f}
(i)$\implies$(iii):
Consider the action of $U$ on $e_i+e_j$ where the $e_i$ are the standard basis vectors and $i\neq j$. Since $U$ preserves the $2$-norm, we have that $\|Ue_i+Ue_j\|^2=2$, or $\langle Ue_i+Ue_j,Ue_i+Ue_j\rangle=2$. Expanding this, we get 
\begin{align*}
    &\langle Ue_i,Ue_i\rangle+\langle Ue_j,Ue_j\rangle+\langle Ue_i,Ue_j\rangle+\conj{\langle Ue_i,Ue_j\rangle}=2\\
    &\implies2+2\text{Re}(\langle Ue_i,Ue_j\rangle)=2\\
    &\implies \text{Re}(\langle Ue_i,Ue_j\rangle)=0
\end{align*}

Repeating this calculation for $e_i+ie_j$ gives that $\text{Im}(\langle Ue_i,Ue_j\rangle)=0$, so the columns of $U$ must be orthogonal to each other. In addition, they must be orthonormal as well because $\inner{Ue_i}{Ue_i}=1$, and from this we have immediately that $U$ is unitary.

(iii)$\implies$(ii): $\inner{Ux}{Uy}=\sum_{i,j}\conj{x_i}y_j\inner{u_i}{u_j}=\sum_i\conj{x_i}y_i=\inner{x}{y}$ since the columns of $U$ are orthonormal.

(ii)$\implies$(i): $\|Ux\|_2^2=\inner{Ux}{Ux}=\inner{x}{x}=\|x\|_2^2$ by (ii).
\subsection*{2}
Examine $\|(I-A)v\|_\alpha=\|v-Av\|_\alpha$. Restrict our consideration to $v$ with norm $1$ (since if $A$ were singular, it'd have to send a whole subspace to $0$, and any nonzero subspace contains some vector with norm $1$). Then, by the triangle inequality, $\|v-Av\|_\alpha\geq\left|1-\|Av\|_\alpha\right|>0$, since $\|Av\|_\alpha$ is less than $1$ because $\|A\|_\alpha<1$. Thus by positive-definiteness, $(I-A)v\neq0$ if $v\neq0$.

By the sub-multiplicativity of the induced norm, we have $\|I-A\|\|(I-A)^{-1}\|\geq1$, and by the triangle inequality we have $\|I-A\|\leq1+\|A\|$, so combining these inequalities gives $\|(I-A)^{-1}\|\geq\frac{1}{1+\|A\|}$. For the other side of the inequality, write $(I-A)^{-1}=I+\sum_{i=1}^\infty A^i$ by analogy with the standard series expansion for $\frac{1}{1-x}$. This converges since $\|A\|<1$, and the difference between arbitrary partial sums is less than the difference between the corresponding partial sum of the geometric series of $\|A\|$. Then, we can take the norm of both sides, obtaining $\|(I-A)^{-1}\|\leq\sum_{i=0}^\infty\|A\|^i=\frac{1}{1-\|A\|}$.
\subsection*{3}
Here are tables recording the relevant values for $n=5$.


\vspace{10pt}
\begin{tabular}{c|c|c|c}
    normal&1-norm&2-norm&inf-norm\\
    \hline
        $\frac{\|\h{x}-x\|}{\|x\|}$&1.998e-16&2.275e-16&3.331e-16\\
        $\kappa(A)$&1.471e+01&5.309e+00&8.624e+00\\
        $\kappa(A)\frac{\|\delta b\|}{\|b\|}$&5.741e-15&2.134e-15&3.588e-15\\
\end{tabular}


\vspace{10pt}
\begin{tabular}{c|c|c|c}
        hilbert&1-norm&2-norm&inf-norm\\
        \hline
            $\frac{\|\h{x}-x\|}{\|x\|}$&2.552e-13&3.493e-13&6.168e-13\\
            $\kappa(A)$&9.437e+05&4.766e+05&9.437e+05\\
            $\kappa(A)\frac{\|\delta b\|}{\|b\|}$&1.623e-11&1.686e-11&4.588e-11\\
\end{tabular}


\vspace{10pt}
\begin{tabular}{c|c|c|c}
        pascal&1-norm&2-norm&inf-norm\\
        \hline
            $\frac{\|\h{x}-x\|}{\|x\|}$&0.000e+00&0.000e+00&0.000e+00\\
            $\kappa(A)$&1.562e+04&8.518e+03&1.562e+04\\
            $\kappa(A)\frac{\|\delta b\|}{\|b\|}$&0.000e+00&0.000e+00&0.000e+00\\
\end{tabular}


\vspace{10pt}
\begin{tabular}{c|c|c|c}
        magic&1-norm&2-norm&inf-norm\\
        \hline
            $\frac{\|\h{x}-x\|}{\|x\|}$&1.554e-16&1.790e-16&2.220e-16\\
            $\kappa(A)$&6.850e+00&5.462e+00&6.850e+00\\
            $\kappa(A)\frac{\|\delta b\|}{\|b\|}$&2.995e-16&5.340e-16&1.498e-15\\
\end{tabular}

The graph for the other dimensions follows. The graphs are ordered Normal, Hilbert, Pascal, Magic going down the rows, and $\frac{\|\h{x}-x\|}{\|x\|}$, $\kappa(A)$, and $\kappa(A)\frac{\|\delta b\|}{\|b\|}$ going across the columns. 

\includegraphics[width=0.95\textwidth]{hw1_files/graph.png}
%TODO: Use log-log plot for everything but pascal
On all the graphs, red indicates the $1$-norm, blue indicates the $2$-norm, and green indicates the $\infty$-norm.

\ssn{Random normal matrices}
Due to the large spread in the data for the normal matrices, it was impossible to get meaningful data on anything but a log plot. By looking at said log plot, it seems that all the parameters increase polynomially with increasing dimension. For the relative error in $x$, there is a consistent trend $\|\cdot\|_1<\|\cdot\|_2<\|\cdot\|_\infty$. In fact, this trend in the norms seems to hold true for all the matrix types. This can be explained by $\delta x$ being usually fairly small, where the norms don't differ much, and the denominator changing rapidly from $n$ to $\sqrt{n}$ to $1$ as we move down the norms. 

The condition number seems to be the lowest for the $2$-norm and about the same for the $1$- and $\infty$- norms, which makes sense because they're calculated basically the same way and the randomness for the normal distribution doesn't care whether you're summing over rows or columns. 

The condition number multiplied by relative error in $b$ follows a similar trend as the condition number, but with more of an increase depending on dimension (quadratically as opposed to linearly?)
\subsection*{4}
\ssn{a}
Suppose $(A-\delta A)$ is singular. Then there exists some $v$ of norm $1$ in its kernel. Examine $\frac{1}{\kappa(A)}$. This is equal to
\begin{align*}
    &\frac{1}{\|A\|\|A^{-1}\|}\\
    =&\frac{1}{\|A\|\max\frac{\|A^{-1}x\|}{\|x\|}}\\
    \leq&\frac{1}{\|A\|\frac{\|A^{-1}Av\|}{\|Av\|}}\\
    =&\frac{\|Av\|}{\|A\|\|v\|}\\
    =&\frac{\|(A-(A+\delta A))v\|}{\|A\|\|v\|}\\
    =&\frac{\|\delta Av\|}{\|A\|\|v\|}\\
    \leq&\frac{\|\delta A\|\|v\|}{\|A\|\|v\|}
\end{align*}
This contradicts the condition that we were given, so no such $v$ may exist.
\ssn{b}
Expanding, we have $b=Ax+A\delta x+\delta Ax + \delta A\delta x=b$, or $A\delta x+\delta A\h{x}=0$, so $\delta x=-A^{-1}\delta A\h{x}$. Taking the norm on both sides and using compatibility gives $\|\delta x\|\leq\|A^{-1}\|\|\delta A\|\|\h{x}\|$, or $\frac{\|\delta x\|}{\|\h{x}\|}\leq\|A^{-1}\|\frac{\|A\|}{\|A\|}\|\delta A\|=\kappa(A)\frac{\|\delta A\|}{\|A\|}$.
\ssn{c}
Starting from the result in (b), multiplying both sides and applying the triangle inequality to $\|\h{x}\|$ gives 
\begin{align*}
    \|\delta x\|&\leq\kappa(A)\frac{\|\delta A\|}{\|A\|}(\|x\|+\|\delta x\|)\\
    \left(1-\kappa(A)\frac{\|\delta A\|}{\|A\|}\right)\|\delta x\|&\leq\kappa(A)\frac{\|\delta A\|}{\|A\|}\|x\|\\
\end{align*}
Since (4.2) guarantees that the LHS is nonzero, we can divide by $\left(1-\kappa(A)\frac{\|\delta A\|}{\|A\|}\right)$ and achieve our desired result.
\subsection*{5}
\ssn{a}
Consider the function $f:\cn^{m\times n}\times\cn^{n\times p}\to\rn$ defined by $f(A,B)=\frac{\|AB\|}{\|A\|\|B\|}$. The range of this function is the same as if we just restricted $A$ and $B$ to have norm $1$, since the scalar multiples factor out of the norm. Thus, since all norms are topologically equivalent, we can view $f$ as a continuous function mapping the product of two unit spheres into $\rn$. Since the unit sphere is compact, the image of $f$ is also compact, which means it's bounded above by some $K\in\rn$. Then, if we let $c>K$, we have $\frac{\|AB\|_c}{\|A\|_c\|B\|_c}=\frac{1}{c}f(A,B)<1$, which means that $\|\cdot\|_c$ is submultiplicative.

For the H\"{o}lder $\infty$ norm, fix $\|\cdot\|=\|\cdot\|_{H,\infty}$. Let the largest element of $A$ be $a$ and the largest element of $B$ be $b$. Then, the largest element of $AB$ has the form $\sum_ka_{ik}b_{kj}\leq\sum_kab=n\|A\|\|B\|$, so we can let $c=n$.
\ssn{b}
We only have to verify it for one Jordan block, since $D_\ep$ can also be seen as a block matrix. Then, let $D_{\ep,r}$ be the diagonal block corresponding to the location of the $r$th Jordan block of size $n_r$. We have
\begin{align*}
    D_{\ep,r}^{-1}J_rD_{\ep,r}&=\openm\ep^{-a}&&\\&\ddots&\\&&\ep^{-(a+n_r)}\closem\openm\lambda_r&1&&\\&\ddots&\ddots&\\&&\ddots&1\\&&&\lambda_r\closem\openm\ep^{a}&&\\&\ddots&\\&&\ep^{(a+n_r)}\closem\\
                              &=\openm\ep^{-a}&&\\&\ddots&\\&&\ep^{-(a+n_r)}\closem\openm\ep^a\lambda_r&\ep^{a+1}&&\\&\ddots&\ddots&\\&&\ddots&\ep^{(a+n_r)}\\&&&\ep^{-(a+n_r)}\lambda_r\closem\\
                              &=\openm\lambda_r&\ep&&\\&\ddots&\ddots&\\&&\ddots&\ep\\&&&\lambda_r\closem
\end{align*}
\ssn{c}
The infinity operator norm is the maximum of the sum along the rows. Since $D_\ep^{-1}JD_\ep$ takes on the same form as the Jordan matrix, the sum along the rows is always either $\lambda_r+\ep$ or $\lambda_r$ for some eigenvalue $\lambda_r$. Since $\rho(J)$ is the largest eigenvalue of $J$, the row sum will never be greater than the largest eigenvalue plus $\ep$.
\ssn{d}
Let $A=P^{-1}JP$, and define $\|x\|=\|D_\ep^{-1}Px\|_\infty$ for $x\in\cn^n$. Then, $\frac{\|Ax\|}{\|x\|}=\frac{\|D_\ep^{-1}PAx\|_\infty}{\|D_\ep^{-1}Px\|_\infty}$. However, note that maximizing this over all $x\in\cn^n$ is equivalent to maximizing this over all $P^{-1}D_\ep x\in\cn^n$, since $D_\ep$ and $P$ are both bijective. Then, we have that the norm induced on $A$ by this vector norm is
\[\|A\|=\max\frac{\|D_\ep^{-1}PAP^{-1}D_\ep x\|_\infty}{\|x\|_\infty}=\|D_\ep^{-1}JD_\ep\|_\infty\]
Now, the maximum row-sum of $D_\ep^{-1}JD_\ep$ will always be at least as large as the largest eigenvalue of $J$, since there is always going to be one row containing solely the largest eigenvalue. Thus, since the eigenvalues of $A$ and $J$ are identical, we have our desired result following from (c).
\end{document}
