\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\renewcommand{\Re}{\mathrm{Re}}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
We have immediately from the definition that $W^\perp$ intersects with $W$ only at zero, as zero is the only vector which has zero inner product with itself. Now, let $w_1,\ldots,w_r$ be an o.n. basis for $W$. For any vector $v\in\cn^n$, let $v'=v-\sum_i\frac{\inner{v}{w_i}}{\inner{w_i}{w_i}}w_i\in W$ be the projection of $v$ onto $W$. Then, taking $\inner{v'}{w_i}$ for any basis vector $w_i$ gives
$\inner{v}{w_i}-\frac{\inner{v}{w_i}}{\inner{w_i}{w_i}}=0$ since $\|w_i\|=1$ by definition. Thus, we can decompose $v$ into a sum of a vector in $W^\perp$ and a vector in $W$.

Now, we have $\cn^n=W\oplus W^\perp$. If we let $X=W^\perp$, then $\cn^n=X\oplus X^\perp=W^\perp\oplus(W^\perp)^\perp$, which tells us that $W$ and $(W^\perp)^\perp$ must have the same dimension. Since we know that $W\subset (W^\perp)^\perp$ because all things in $W$ are orthogonal to all things in $W^\perp$, we therefore have equality.
\ssn{b}
Let $v\in\cn^m$ such that $A^*v=0$, and $w=Ax$ for some $x\in\cn^n$. Then, $\inner{v}{w}=\inner{v}{Ax}=\inner{A^*v}{x}=\inner{0}{x}=0$ for all such pairs $v,w$, showing that $\ker(A^*)=\im(A)^\perp$. Similarly, if we let $v=A^*x$ for some $x\in\cn^m$ and $w\in\cn^n$ be such that $Aw=0$, we have $\inner{w}{v}=\inner{w}{A^*x}=\inner{Ax}{x}=0$.
\ssn{c}
$\ker(A^*)$ and $\im(A)$ are subspaces of $\cn^m$ that are orthogonal complements of each other, so by (a) their direct sum is equal to $\cn^m$. Similarly, $\ker(A)$ and $\im(A^*)$ are orthogonally complementary subspacse of $\cn^n$, so $\cn^n$ equals their direct sum.
\ssn{d}
We begin by noting that if $x\in V^\perp$, then the projection of $x$ onto $V$ is the zero vector, since the projection operator is equal to $\sum_i\inner{x}{v_i}v_i$ for some o.n. basis $\{v_i\}$ of $V$.
\begin{align*}
    P_{\ker(A)}x&=P_{\ker(A)}x_0+P_{\ker(A)}x_1=x_0\\
    P_{\im(A^*)}x&=P_{\im(A^*)}x_0+P_{\im(A^*)}x_1=x_1\\
    P_{\ker(A^*)}y&=P_{\ker(A^*)}y_0+P_{\ker(A^*)}y_1=y_0\\
    P_{\im(A)}y&=P_{\im(A)}y_0+P_{\im(A)}y_1=y_1\\
\end{align*}
\ssn{e}
Write $b=b_0+b_1$, where $b_0\in\ker(A^*)$ and $b_i\in\im(A)$. Then, $Ax-b=(Ax-b_1)+(-b_0)$, where $Ax-b_1\in\im(A)$. Then, since $\im(A)$ and $\ker(A^*)$ are orthogonal, we have $\|Ax-b\|_2^2=\|(Ax-b_1)\|_2^2+\|b_0\|_2^2\geq\|b_0\|_2^2$, with equality iff $\|Ax-b_1\|_2^2=0$, or $Ax=b_1$. Since we are guaranteed that $\|Ax-b\|_2$ is minimized when we have equality, any solution $x$ to the min least squares problem must satisfy $Ax=b_1$.
\ssn{f}
(1.2)$\implies$(1.3): Multiply on both sides by $A^*$ to get $A^*Ax=A^*b_1$, or $A^*Ax=A^*b_1+A^*b_0=A^*b$ since $b_0\in\ker(A^*)$.

(1.3)$\implies$(1.2): Write $b$ as $b_0+b_1$ so we have $A^*Ax=A^*b_0+A^*b_1=A^*b_1$. Rearrange to produce $A^*(Ax-b_1)=0$, or $Ax-b_1\in\ker(A^*)$. However, since $Ax$ and $b_1$ are both in the image of $A$, their diff can only be in the kernel of $A^*$ if it's zero, so we have $Ax-b_1=0$.
\ssn{g}
Let $x$ be such that $Ax=b_1$, and write $x=x_0+x_1$ where $x_0\in\ker(A)$ and $x_1\in\im(A^*)$, so we have $P_{\im(A^*)}x=x_1$. Taking the norm of $x$ squared, we have $\|x\|_2^2=\|x_0\|_2^2+\|x_1\|_2^2\geq x_1$. Now note that $Ax=A(x_0+x_1)=Ax_1$, so in order to minimize the norm of $x$, we are free to set $x_0$ to anything we want, so let's just make it $0$. Then, we reach the lower bound for the norm of $x$, and this is attained when $x$ lies in $\im(A^*)$. 
\ssn{h}
For the kernel, $\ker(A)=\ker(A^*A)=\ker(AA^*)=\ker(A^*)$. For the image, we have $\im(A)=\im(AA^*)=\im(A^*A)=\im(A)$. Then by the Fredholm alternative, $\cn^n=\ker(A)\oplus\im(A^*)=\ker(A)\oplus\im(A)$. 
\subsection*{2}
\ssn{a}
From HW1, we know that $\|A\|_F^2=\tr(A^*A)$, so 
\begin{align*}
    \|A-BX\|_F^2&=\tr((A-BX)^*(A-BX))\\
    &=\tr((A^*-X^*B^*)(A-BX))&\\
    &=\tr(A^*A-A^*BX-X^*B^*A+X^*B^*BX)\\
    &=\tr(A^*A)-\tr((X^*B^*A)^*)-\tr(X^*B^*A)+\tr(X^*B^*BX)
\end{align*}
Since the trace of the conj. transpose of a matrix is just the conjugate of the trace, the middle two terms collapse to $2\Re(\tr(X^*B^*A))$. Also, since trace is invariant under change of basis, the last term becomes just $\tr(B^*B)$, which results in the expression that we wanted. Since the only term that depends on $X$ is $\Re(\tr(X^*B^*A))$, we might as well just maximize this instead.
\ssn{b}
Write $X^*B^*A=X^*U\Sigma V$ by taking the SVD of $B^*A$. Then, since the trace is invariant under cyclic permutations, we have $\tr(X^*B^*A)=\tr(\Sigma V^*X^*U)=\tr(\Sigma\Lambda)$ where $\Lambda$ is unitary. Working out the matrix multiplication, the diagonal entries of $\Sigma\Lambda$ are of the form $\sigma_i\lambda_{ii}$. Since each column of $\Lambda$ has norm $1$, every entry of $\Lambda$ is at most norm $1$, so $\Re(\sigma_i\lambda_{ii})$ is bounded above by $\Re(\sigma_i)$. Thus, $\Re(\tr(\Sigma\Lambda))\leq\Re(\tr(\Sigma))=\sum_i\sigma_i(B^*A)$, which is what we wanted. Equality is attained when $\Lambda=I_n$, or when $V^*X^*U=I_n\implies X=UV^*$. 
\ssn{c}
Since from above, we have that the minimum is attained when $X=UV^*$, plugging into the formula in (a) gives $\tr(A^*A)+\tr(B^*B)-2\sum\sigma_i(B^*A)$. Now, we know from class that $\tr(A^*A)$ is the sum of the squares of the singular values of $A$, and same with $B$. Putting these all together and accounting for possibly zero singular values, we get the formula we were supposed to prove.
\ssn{d}
(i): The unitariness of $V$ and $U$ is given by the existence of the SVD. Since $A$ is full-rank and $n\leq m$, it must have $n$ nonzero singular values.

\noindent(ii): Use the fact that left- or right- multiplying by a unitary matrix does not change the Frobenius norm. Then, we have
\begin{align*}
    \|AX-B\|_F^2&=\left\|U\openm\Sigma\\0\closem V^*X-B\right\|_F^2\\
                &=\left\|\openm\Sigma\\0\closem V^*X-U^*B\right\|_F^2\\
                &=\left\|\openm\Sigma\\0\closem V^*XV-U^*BV\right\|_F^2\\
                &=\left\|\openm\Sigma\\0\closem V^*XV-\openm C_1\\C_2\closem\right\|_F^2\\
\end{align*}
Since we can just split up the Frobenius norm into concatenations of vectors (or matrices here), this is equal to $\norm{\Sigma V^*XV-C_1}_F^2+\norm{C_2}_F^2$, which is the desired result.

\noindent(iii): The first term is due to the sum of squares along the diagonals of $\Sigma Y-C_1$. The rest of it is $\sum_{j\neq i}|\sigma_iy_{ij}-c_{ij}|^2$, but since $Y$ is Hermitian, we can pair up each term with the corresponding term flipped across the diagonal and get $\sum_{j>i}|\sigma_iy_{ij}-c_{ij}|^2+|\sigma_j\conj{y_{ij}}-c_{ji}|^2=\sum_{j>i}|\sigma_iy_{ij}-c_{ij}|^2+|\sigma_jy_{ij}-\conj{c_{ji}}|^2$. 

We can minimize this term-by-term, since each of the terms depends only on one $y_{ij}$. Let $a=\sigma_i, b=c_{ij}, c=\sigma_j, d=\conj{c_{ji}}$ (with $a,c\in\rn$). Then, the problem is to minimize $|az-b|^2+|cz-d|^2$, or $a^2|z-b/a|^2+c^2|z-d/c|^2$. Thus, $z$ must lie on the line from $b/a$ to $d/c$. Suppose $z$ is at distance $l$ from $b/a$, and let $L=|b/a-d/c|$. Then, we want to minimize $a^2l^2+c^2(L-l)^2$. Differentiating wrt $l$ and minimizing this way, we obtain $l=\frac{c^2L}{c^2+a^2}$. Now, since we have $z=b/a+\frac{b/a-d/c}{L}l$, this becomes $\frac{b}{a}+\frac{b/a-d/c}{c^2+a^2}=\frac{ab+cd}{a^2+c^2}$ (maybe), which is the desired result.
\ssn{e}
For $\det(A)=0$, just let $X=A$. Otherwise, $A$ is invertible, and thus has a SVD $U\Sigma V^*$ where $\Sigma$ is diagonal and invertible. Then, $|\det(A)|=\det(\Sigma)$, so all we have to do is find some $X$ that has determinant equal to that of $\Sigma$.

We have $\norm{A-X}_F^2=\norm{U\Sigma V^*-X}_F^2=\norm{\Sigma-U^*XV}_F^2=\norm{\Sigma-D}_F^2$, where $D$ is defined appropriately. If we let $\det(U)=u$ and $\det(V)=v$, then we must have $\det(X)=\frac{u}{v}\det(D)$, or $\det(D)=\frac{v}{u}|\det(A)|$. The problem now becomes finding such a $D$ that minimizes $\norm{\Sigma-D}_F^2$. 

Expand this expression: 
\begin{align*}
    \norm{D-\Sigma}_F^2&=\sum_i(d_{ii}-\sigma_i)^2+\sum_{i\neq j}d_{ij}^2\\
                       &=\sum_i(d_{ii}^2-2d_{ii}\sigma_i+\sigma_i^2)+\sum_{i\neq j}d_{ij}^2\\
                       &=\sum_i(\sigma_i^2-2d_{ii}\sigma_i)+\norm{D}_F^2
\end{align*}
If we want to minimize the expression found above for $\norm{D-\Sigma}_F^2$, for any fixed $\norm{D}_F^2$, we need to have the $d_{ii}$ maximized. The best way to do this is simply to make $D$ diagonal, so we have $X=UDV^*$ as the SVD of $X$ now. Now we need to find some $D$ that makes $X$ have the correct determinant.

Ideally, we'd want the entries of $D$ to be the same as $\Sigma$, but we need to have the product of the entries of $D$ equal $\frac{v}{u}|\det(A)|$. Instead, we can just have most of $D$ be the same as $\Sigma$, but change the smallest entry to get the appropriate multiplier of $\frac{v}{u}$ in there.

The problem now becomes to find some appropriate $\Delta$. Evidently, we must have $\prod_i\delta_{ii}=|\det(A)|$
Let $W\Delta T^*$ be the SVD of any matrix $X$ with the correct determinant. Then, $D=U^*W\Delta T^*V$, so $\norm{D}_F^2=\norm{\Delta}_F^2$.
\subsection*{3}
\ssn{a}
Suppose $\rank(A)=1$. Then the image of $x$ as a function $\cn\to\cn^m$ must have dimension $1$, so $x$ is nonzero. Further, if $y$ is zero, $xy^*$ would be zero, which would mean that the rank of $A$ is zero. Now suppose $x$ and $y$ are nonzero. Then $y^*$ as a function $\cn^n\to\cn$ is surjective, so the image of $A$ is the image of $x$, which has dimension $1$ since $x$ is nonzero.
\ssn{b}
We have $A^*A=yx^*xy^*=\|x\|_2^2yy^*$. Since $yy^*$ has rank $1$, it has only one nonzero eigenvalue, which conveniently happens to be when it's applied to $y$ with value $\|y\|_2^2$. Thus, the $2$-norm of $A$ is the square root of $\|x\|^2_2\|y\|_2^2$, or $\|x\|\|y\|$. 

The Frobenius norm squared of $A$ is $\sum_i\sum_j|x_i\conj{y}_j|^2=\sum_i\sum_j|x_i|^2|y_j|^2=\|x\|^2\|y\|^2$, so this is equal to the above as well after taking a sqrt.

The infinity norm of $A$ is the max row-sum, or $\max_i\left(\sum_j|x_i||\conj{y}_j|\right)\leq\|x\|_\infty\max_i\left(\sum_j|y_j|\right)=\|x\|_\infty\|y\|_1$.

By a similar argument with the indices flipped around, $\|A\|_1\leq\|x\|_1\|y\|_\infty$.
\ssn{c}
Let $X$ be a $m\times r$ matrix with columns consisting of the $x_i$, and construct $Y$ similarly. Let $X=QR$ and $Y=ST$ using the reduced QR decomposition. Since all the $x_i$ and $y_i$ are independent, $R$ and $T$ are invertible. Thus, we have $x_i=Rq_i$ and $y_i=Ts_i$. Subsituting into the expression for $A$, we have $A=R(q_1s_1^*+\cdots+q_rs_r^*)T^*$. Since $R$ and $T$ are invertible, the rank of $A$ is the rank of the inner sum. Denote it by $K$. 

Now, note that for any $i$ and $v\in\cn^n$, $q_is_i^*v$ is a scalar multiple of $q_i$. Due to the independence of the $q_i$, if not all of the $s_i^*v$ are zero, then the sum cannot be zero. Now suppose that $s_i^*v=0$ for all $i$. This is equivalent to $\langle s_i,v\rangle=0$ for all $i$, or $v\in\bigcap_is_i^\perp$. Since the $s_i$ are orthogonal, their orthogonal complements are also orthogonal, so the intersection of $r$ of those should have dimension $n-r$. Thus, the kernel of $K$ has dimension $n-r$, which means that the rank of $K$ is $r$. (Yes, I'm aware I didn't need to do the orthogonality thing)

For specific counterexamples, consider $\openm0\\1\closem\openm1&1\closem+\openm0\\2\closem\openm1&-1\closem=\openm0&0\\1&1\closem+\openm0&0\\2&-2\closem$, which cannot be rank $2$ because the first row is all zero. Also consider $\openm1\\-1\closem\openm0&1\closem+\openm1\\1\closem\openm0&2\closem=\openm0&1\\0&-1\closem+\openm0&2\\0&2\closem$, which is again not rank $2$.
\ssn{d}
Suppose $A$ can be written as a sum of $k$ rank-1 matrices. Let $A=x_1y_1^*+\cdots+x_ky_k^*$. Then $\im(A)\subset\bigcup\Span(x_1,\ldots,x_k)$ by the same argument as (c), which implies $\rank(A)\leq k$. Now, since $A$ can always be written as $\sum_i\sigma_iu_iv_i^*$ where $u_i$ and $v_i$ are the left- and right- singular vectors of $A$, the lower bound is attained since the rank of $A$ is the number of nonzero singular values.
\ssn{e}
We have $\|A\|_F=\sqrt{\sigma_1^2+\cdots+\sigma_r^2}\leq\sqrt{r\sigma_1^2}=\sqrt{r}\sigma_1=\sqrt{r}\|A\|_2$
\ssn{f}
The nuclear norm is $\|A\|_*=\sigma_1+\cdots+\sigma_r\leq r\sigma_1=r\|A\|_2$.
\subsection*{4}
Let $X_*$ be a minimum-rank matrix subject to the entry-wise constraints. Let $\rank(X_*)=k$, so $X_*$ has $k$ nonzero singular values. Thus, $f_r(X_*)=0$ iff $r\geq k$. 

Now, let $X_r$ be a minimizer of (4.9). If $r\geq k$, then we have that $X_*$ satisfies the constraints. Since $f_r(X_*)=0$, any mimimizer of (4.9) must also satisfy $f_r(X_r)=0$. Conversely, if the minimum value of $f_r$ subject to the constraints is $0$, then $X_r$ can have no more than $r$ nonzero singular values, implying that the rank of $X_r$ is at most $r$. However, since $X_r$ satisfies the constraints in (4.8), we must have $\rank(X_*)\leq\rank(X_r)\leq r$.

With this, we can now solve (4.8) by solving (4.9) for increasing $r$ (which we presumably know how to do since it's a continuous problem), and stopping when we get a min value of zero. However, since there are iterative algorithms involved, it's a bit difficult to determine when the min value is actually zero, so I assume that's the reason for the parenthetical ``in principle'' in the problem statement
\subsection*{5}
\ssn{a}
We have $\|A\|_F^2=\tr(A^*A)$, so
\begin{align*}
    \left\|A\left(I-\frac{xx^*}{x^*x}\right)\right\|_F^2&=\tr\left(\left(I-\frac{xx^*}{x^*x}\right)A^*A\left(I-\frac{xx^*}{x^*x}\right)\right)\\
                                                    &=\tr(A^*A)-2\frac{\tr(A^*Axx^*)}{x^*x}+\frac{\tr(xx^*A^*Axx^*)}{(x^*x)^2}\\
\end{align*}
We recognize the first term as $\|A\|_F^2$. The numerator in the last term is $\tr((Axx^*)^*(Axx^*))=\|Axx^*\|_F^2$, which by (3a) is $\|Ax\|_2^2\|x\|_2^2$. Canceling with the denominator gives the last term as $\frac{\|Ax\|_2^2}{\|x\|_2^2}$.

Since trace is invariant under cyclic permutation, the numerator of the second term can be written as $\tr(Axx^*A^*)=\tr((Ax)(Ax)^*)=\|Ax\|_F^2=\|Ax\|_2^2$. Putting it all together, we have that the expression is equal to $\|A\|_F^2-\frac{\|Ax\|_2^2}{\|x\|_2^2}$.
\ssn{b}
Suppose $(A+E)x=b$. Then $Ex=b-Ax$ implying $\|Ex\|_2=\|b-Ax\|_2$. By the defn of the $2$-norm, $\|E\|_2\geq\frac{\|b-Ax\|_2}{\|x\|_2}$. 

Now consider $E^*=\frac{(b-Ax)x^*}{x^*x}$. By (3a), $\|E^*\|_2=\frac{\|b-Ax\|_2\|x\|_2}{\|x\|_2^2}=\frac{\|b-Ax\|_2}{\|x\|_2}$, which attains the lower bound shown above. 
\ssn{c}
By (b), any pair $x,E$ where $E=\frac{(b-Ax)x^*}{\|x\|_2^2}$ is consistent. Further, this $E$ has the minimum $2$-norm of all possible $E$ that make the system consistent with $x$. Thus, to minimize over all pairs $x,E$, we just need to find some $x$ such that the $2$-norm of $\frac{(b-Ax)x^*}{\|x\|_2^2}$ is minimized. We found the $2$-norm to be $\frac{\|b-Ax\|_2}{\|x\|_2}$ above. 

At this point, we need to assume that $A$ is injective, for otherwise we could just take $x\in\ker(A)$ and multiply it by some huge scalar, which will make the $2$-norm arbitrarily close to zero. 
%TODO: solve this.

As for the Frobenius norm, the $E$ from (b) was a rank-1 matrix, which means that its $2$-norm is the same as its Frobenius norm. Since the Frobenius norm of any matrix is always bounded below by the $2$-norm, the solution to the $2$-norm minimization is the same as that of the Frobenius norm minimization.
\ssn{d}
Minimum: let $E'=\frac{ba^*}{\|a\|_2^2}$. Then $E'a=\frac{ba^*a}{\|a\|_2^2}=b$, so if $\|E'\|_F\leq\delta$, we can just take $E'$ to be the minimizer. Evaluating $\|E'\|_F$, we have by (3a) that it's $\frac{\|b\|_2\|a\|_2}{\|a\|_2^2}=\frac{\|b\|_2}{\|a\|_2}$. Thus, let us restrict our attention to the case where $\frac{\|b\|_2}{\|a\|_2}>\delta$, since otherwise we can just take $E'$ to be our solution.

Now consider any matrix $E$ with $\|E\|_F\leq\delta$. Then, $\|Ea\|_2\leq\|E\|_2\|a\|_2\leq\|E\|_F\|a\|_2\leq\delta\|a\|_2<\|b\|_2$ by the above consideration. Then, we have by the triangle inequality $\|Ea-b\|_2\geq\left|\|Ea\|_2-\|b\|_2\right|=\|b\|_2-\|Ea\|_2\geq\|b\|_2-\delta\|a\|_2$. Now, let $E'=\delta\frac{ba^*}{\|b\|_2\|a\|_2}$. We have $\|E'\|_F=\delta\frac{\|b\|_2\|a\|_2}{\|b\|_2\|a\|_2}=\delta$, and
\begin{align*}
    \|E'a-b\|_2&=\left\|\delta\frac{ba^*a}{\|b\|_2\|a\|_2}-b\right\|_2\\
              &=\left\|\delta b\frac{\|a\|_2}{\|b\|_2}-b\right\|_2\\
              &=\|b\|_2\left|\delta\frac{\|a\|_2}{\|b\|_2}-1\right|\\
              &=\left|\delta\|a\|_2-\|b\|_2\right|
\end{align*}
which attains the lower bound found above.

For the maximum, for any $E$ which satisfies the constraints, we have $\|Ea-b\|_2\leq\|Ea\|_2+\|b\|_2\leq\delta\|a\|_2+\|b\|_2$. If we let $E'=-\delta\frac{ba^*}{\|b\|_2\|a\|_2}$ and follow the calculation through from above, we get $\|E'a-b\|_2=\delta\|a\|_2+\|b\|_2$, which attains the upper bound.
\subsection*{6}
\ssn{a}
Projecting a vector onto another vector is just calculating $\frac{a^Tb}{\|b\|}$. If we have the SVD of $A$, $A=U\Sigma V^*$, then $AV=U\Sigma$. The inner product of the $i$th row of $A$ with the $j$th right-singular vector is then just $(U\Sigma)_{ij}$, or $\sigma_ju_{ij}$, and the inner product of the $j$th column of $A$ with the $i$th left-singular vector is similarly $(\Sigma V^*)_{ij}=\sigma_i \conj{v_{ji}}$. 

Projecting onto the first two right-singular vectors as the $x$ and $y$ axes respectively, we have that the $x$-value of the $i$th row is $\sigma_1u_{i1}$ and the $y$-value is $\sigma_1v_{1i}$. The plot follows.

\includegraphics[width=0.9\textwidth]{hw2_files/countries.png}

Hong Kong and Singapore are the two obvious outliers.
\ssn{b}
Using the same technique as above, we have

\includegraphics[width=0.9\textwidth]{hw2_files/features.png}

where the two points stacked in the lower half are \verb|Population/sq.km| and \verb|Population/1000HectarAgri|.
\ssn{c}
The overlaid plot is below:

\includegraphics[width=0.9\textwidth]{hw2_files/overlay.png}

We can see that the two demographic variables \verb|Population/sq.km| and \verb|Population/1000HectarAgri| are located near the two outliers from before. We can interpret this as follows: Write $\hat{A}$ as $\sigma_1u_1v_1^T+\sigma_2u_2v_2^T$, where $\hat{A}$ represents the rank 2 approximation to $A$. The $ij$th entry of $A$ is approximated by the $ij$th entry of $\hat{A}$, which is in turn $\sigma_1u_{i1}v_{1j}^T+\sigma_2u_{i2}v_{2j}$. Note that this is very similar to the inner product of the $i$th country point with the $j$th feature point in the biplot (and would be exactly the same had we used $\sqrt{\sigma_i}$ to do scaling). The fact that HK and Singapore are so close to the population metrics so far out indicates that those two stats are responsible for the two countries being outliers, since their dot product is likely to have extreme values.
\ssn{d}
I removed USA as well because it was making Japan's name unreadable. From this reduced plot, it looks like the countries most similar to Japan are Austria and Poland. 

\includegraphics[width=0.8\textwidth]{hw2_files/countries_no_outliers.png}
\end{document}
