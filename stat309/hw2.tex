\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\renewcommand{\Re}{\mathrm{Re}}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
We have immediately from the definition that $W^\perp$ intersects with $W$ only at zero, as zero is the only vector which has zero inner product with itself. Now, let $w_1,\ldots,w_r$ be an o.n. basis for $W$. For any vector $v\in\cn^n$, let $v'=v-\sum_i\frac{\inner{v}{w_i}}{\inner{w_i}{w_i}}w_i\in W$ be the projection of $v$ onto $W$. Then, taking $\inner{v'}{w_i}$ for any basis vector $w_i$ gives
$\inner{v}{w_i}-\frac{\inner{v}{w_i}}{\inner{w_i}{w_i}}=0$ since $\|w_i\|=1$ by definition. Thus, we can decompose $v$ into a sum of a vector in $W^\perp$ and a vector in $W$.

Now, we have $\cn^n=W\oplus W^\perp$. If we let $X=W^\perp$, then $\cn^n=X\oplus X^\perp=W^\perp\oplus(W^\perp)^\perp$, which tells us that $W$ and $(W^\perp)^\perp$ must have the same dimension. Since we know that $W\subset (W^\perp)^\perp$ because all things in $W$ are orthogonal to all things in $W^\perp$, we therefore have equality.
\ssn{b}
Let $v\in\cn^m$ such that $A^*v=0$, and $w=Ax$ for some $x\in\cn^n$. Then, $\inner{v}{w}=\inner{v}{Ax}=\inner{A^*v}{x}=\inner{0}{x}=0$ for all such pairs $v,w$, showing that $\ker(A^*)=\im(A)^\perp$. Similarly, if we let $v=A^*x$ for some $x\in\cn^m$ and $w\in\cn^n$ be such that $Aw=0$, we have $\inner{w}{v}=\inner{w}{A^*x}=\inner{Ax}{x}=0$.
\ssn{c}
$\ker(A^*)$ and $\im(A)$ are subspaces of $\cn^m$ that are orthogonal complements of each other, so by (a) their direct sum is equal to $\cn^m$. Similarly, $\ker(A)$ and $\im(A^*)$ are orthogonally complementary subspacse of $\cn^n$, so $\cn^n$ equals their direct sum.
\ssn{d}
We begin by noting that if $x\in V^\perp$, then the projection of $x$ onto $V$ is the zero vector, since the projection operator is equal to $\sum_i\inner{x}{v_i}v_i$ for some o.n. basis $\{v_i\}$ of $V$.
\begin{align*}
    P_{\ker(A)}x&=P_{\ker(A)}x_0+P_{\ker(A)}x_1=x_0\\
    P_{\im(A^*)}x&=P_{\im(A^*)}x_0+P_{\im(A^*)}x_1=x_1\\
    P_{\ker(A^*)}y&=P_{\ker(A^*)}y_0+P_{\ker(A^*)}y_1=y_0\\
    P_{\im(A)}y&=P_{\im(A)}y_0+P_{\im(A)}y_1=y_1\\
\end{align*}
\ssn{e}
Write $b=b_0+b_1$, where $b_0\in\ker(A^*)$ and $b_i\in\im(A)$. Then, $Ax-b=(Ax-b_1)+(-b_0)$, where $Ax-b_1\in\im(A)$. Then, since $\im(A)$ and $\ker(A^*)$ are orthogonal, we have $\|Ax-b\|_2^2=\|(Ax-b_1)\|_2^2+\|b_0\|_2^2\geq\|b_0\|_2^2$, with equality iff $\|Ax-b_1\|_2^2=0$, or $Ax=b_1$. Since we are guaranteed that $\|Ax-b\|_2$ is minimized when we have equality, any solution $x$ to the min least squares problem must satisfy $Ax=b_1$.
\ssn{f}
(1.2)$\implies$(1.3): Multiply on both sides by $A^*$ to get $A^*Ax=A^*b_1$, or $A^*Ax=A^*b_1+A^*b_0=A^*b$ since $b_0\in\ker(A^*)$.

(1.3)$\implies$(1.2): Write $b$ as $b_0+b_1$ so we have $A^*Ax=A^*b_0+A^*b_1=A^*b_1$. Rearrange to produce $A^*(Ax-b_1)=0$, or $Ax-b_1\in\ker(A^*)$. However, since $Ax$ and $b_1$ are both in the image of $A$, their diff can only be in the kernel of $A^*$ if it's zero, so we have $Ax-b_1=0$.
\ssn{g}
Let $x$ be such that $Ax=b_1$, and write $x=x_0+x_1$ where $x_0\in\ker(A)$ and $x_1\in\im(A^*)$, so we have $P_{\im(A^*)}x=x_1$. Taking the norm of $x$ squared, we have $\|x\|_2^2=\|x_0\|_2^2+\|x_1\|_2^2\geq x_1$. Now note that $Ax=A(x_0+x_1)=Ax_1$, so in order to minimize the norm of $x$, we are free to set $x_0$ to anything we want, so let's just make it $0$. Then, we reach the lower bound for the norm of $x$, and this is attained when $x$ lies in $\im(A^*)$. 
\ssn{h}
For the kernel, $\ker(A)=\ker(A^*A)=\ker(AA^*)=\ker(A^*)$. For the image, we have $\im(A)=\im(AA^*)=\im(A^*A)=\im(A)$. Then by the Fredholm alternative, $\cn^n=\ker(A)\oplus\im(A^*)=\ker(A)\oplus\im(A)$. 
\subsection*{2}
\ssn{a}
From HW1, we know that $\|A\|_F^2=\tr(A^*A)$, so 
\begin{align*}
    \|A-BX\|_F^2&=\tr((A-BX)^*(A-BX))\\
    &=\tr((A^*-X^*B^*)(A-BX))&\\
    &=\tr(A^*A-A^*BX-X^*B^*A+X^*B^*BX)\\
    &=\tr(A^*A)-\tr((X^*B^*A)^*)-\tr(X^*B^*A)+\tr(X^*B^*BX)
\end{align*}
Since the trace of the conj. transpose of a matrix is just the conjugate of the trace, the middle two terms collapse to $2\Re(\tr(X^*B^*A))$. Also, since trace is invariant under change of basis, the last term becomes just $\tr(B^*B)$, which results in the expression that we wanted. Since the only term that depends on $X$ is $\Re(\tr(X^*B^*A))$, we might as well just maximize this instead.
\ssn{b}
Write $X^*B^*A=X^*U\Sigma V$ by taking the SVD of $B^*A$. Then, since the trace is invariant under cyclic permutations, we have $\tr(X^*B^*A)=\tr(\Sigma V^*X^*U)=\tr(\Sigma\Lambda)$ where $\Lambda$ is unitary. Working out the matrix multiplication, the diagonal entries of $\Sigma\Lambda$ are of the form $\sigma_i\lambda_{ii}$. Since each column of $\Lambda$ has norm $1$, every entry of $\Lambda$ is at most norm $1$, so $\Re(\sigma_i\lambda_{ii})$ is bounded above by $\Re(\sigma_i)$. Thus, $\Re(\tr(\Sigma\Lambda))\leq\Re(\tr(\Sigma))=\sum_i\sigma_i(B^*A)$, which is what we wanted. Equality is attained when $\Lambda=I_n$, or when $V^*X^*U=I_n\implies X=UV^*$. 
\ssn{c}
Since from above, we have that the minimum is attained when $X=UV^*$, plugging into the formula in (a) gives $\tr(A^*A)+\tr(B^*B)-2\sum\sigma_i(B^*A)$. Now, we know from class that $\tr(A^*A)$ is the sum of the squares of the singular values of $A$, and same with $B$. Putting these all together and accounting for possibly zero singular values, we get the formula we were supposed to prove.
\ssn{d}
(i): The unitariness of $V$ and $U$ is given by the existence of the SVD. Since $A$ is full-rank and $n\leq m$, it must have $n$ nonzero singular values.

\noindent(ii): Use the fact that left- or right- multiplying by a unitary matrix does not change the Frobenius norm. Then, we have
\begin{align*}
    \|AX-B\|_F^2&=\left\|U\openm\Sigma\\0\closem V^*X-B\right\|_F^2\\
                &=\left\|\openm\Sigma\\0\closem V^*X-U^*B\right\|_F^2\\
                &=\left\|\openm\Sigma\\0\closem V^*XV-U^*BV\right\|_F^2\\
                &=\left\|\openm\Sigma\\0\closem V^*XV-\openm C_1\\C_2\closem\right\|_F^2\\
\end{align*}
Since we can just split up the Frobenius norm into concatenations of vectors (or matrices here), this is equal to $\norm{\Sigma V^*XV-C_1}_F^2+\norm{C_2}_F^2$, which is the desired result.

\noindent(iii): The first term is due to the sum of squares along the diagonals of $\Sigma Y-C_1$. The rest of it is $\sum_{j\neq i}|\sigma_iy_{ij}-c_{ij}|^2$, but since $Y$ is Hermitian, we can pair up each term with the corresponding term flipped across the diagonal and get $\sum_{j>i}|\sigma_iy_{ij}-c_{ij}|^2+|\sigma_j\conj{y_{ij}}-c_{ji}|^2=\sum_{j>i}|\sigma_iy_{ij}-c_{ij}|^2+|\sigma_jy_{ij}-\conj{c_{ji}}|^2$. 

We can minimize this term-by-term, since each of the terms depends only on one $y_{ij}$. Let $a=\sigma_i, b=c_{ij}, c=\sigma_j, d=\conj{c_{ji}}$ (with $a,c\in\rn$). Then, the problem is to minimize $|az-b|^2+|cz-d|^2$, or $a^2|z-b/a|^2+c^2|z-d/c|^2$. Thus, $z$ must lie on the line from $b/a$ to $d/c$. Suppose $z$ is at distance $l$ from $b/a$, and let $L=|b/a-d/c|$. Then, we want to minimize $a^2l^2+c^2(L-l)^2$. Differentiating wrt $l$ and minimizing this way, we obtain $l=\frac{c^2L}{c^2+a^2}$. Now, since we have $z=b/a+\frac{b/a-d/c}{L}l$, this becomes $\frac{b}{a}+\frac{b/a-d/c}{c^2+a^2}=\frac{ab+cd}{a^2+c^2}$ (maybe), which is the desired result.
\ssn{e}
For $\det(A)=0$, just let $X=A$. Otherwise, $A$ is invertible, and thus has a SVD $U\Sigma V^*$ where $\Sigma$ is diagonal and invertible. Then, $|\det(A)|=\det(\Sigma)$, so all we have to do is find some $X$ that has determinant equal to that of $\Sigma$.

We have $\norm{A-X}_F^2=\norm{U\Sigma V^*-X}_F^2=\norm{\Sigma-U^*XV}_F^2=\norm{\Sigma-D}_F^2$, where $D$ is defined appropriately. If we let $\det(U)=u$ and $\det(V)=v$, then we must have $\det(X)=\frac{u}{v}\det(D)$, or $\det(D)=\frac{v}{u}|\det(A)|$. The problem now becomes finding such a $D$ that minimizes $\norm{\Sigma-D}_F^2$. 


Let $WDT^*$ be the SVD of any matrix $X$ with the correct determinant. Then, we must have $\det(D)=\det(X)$
\end{document}
