\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\nc{\ta}{\theta}
\nc{\vp}{\varphi}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
For the first coordinate, multiplying the matrix by the given vector gives $\alpha\sin(j\ta)-\sin(2j\ta)=\sin(j\ta)(\alpha-2\cos(j\ta))$. For the last coordinate, we get $-\sin((n-1)j\ta)+\alpha\sin(nj\ta)=\alpha\sin(nj\ta)-\sin(nj\ta)\cos(j\ta)+\cos(nj\ta)\sin(j\ta)$, but since $\cos(nj\ta)=\cos(j\ta)$ and $\sin(nj\ta)=-\sin(j\ta)$, we have that this is $\sin(nj\ta)(\alpha-2\cos(j\ta))$.

For all the other coordinates, we have that the $i$th coordinate is $\alpha\sin(ij\ta)-\sin((i-1)j\ta)-\sin((i+1)j\ta)$. Breaking up the last two terms with trig, we get 
\[\alpha\sin(ij\ta)-\sin(ij\ta)\cos(j\ta)+\sin(j\ta)\cos(ij\ta)-\sin(ij\ta)\cos(j\ta)-\sin(j\ta)\cos(ij\ta)=\sin(ij\ta)(\alpha-2\cos(j\ta)\]
as desired. Thus the given vector is an eigenvector for $T_\alpha$ with eigenvalue $\alpha-2\cos(j\ta)$. The requirement that $e^{j\ta}$ is conjugate to $e^{nj\ta}$ (imposed by the last coordinate) dictates that $j$ must run from $1..n$ and $\theta=\frac{\pi}{n+1}$. Thus we have a list of eigenvalues and eigenvectors.

As long as $\alpha>2\cos(\frac{\pi}{n+1})$, the matrix will be positive definite because cosine takes on its greatest value for low values of its argument.
\ssn{b}
The iteration matrix is a matrix with zeros everywhere except the superdiagonal and the subdiagonal, whose entries are all $\frac{1}{2}$. To compute the $2$-norm, note that it has the same form as $T_\alpha$, so its eigenvalues are $\cos(j\theta)$, and so the $2$-norm is the largest one, or $\cos(\theta)$. Since this is less than $1$, the iteration will converge. Now, since this is a symmetric matrix, it's orthogonally diagonalizable, so $\|B_\alpha^m\|=\|B_\alpha\|^m$, which gives us the rate of convergence as $\lim_{m\to\infty}\frac{-\log(\cos(\theta)^m)}{m}=-\log(\cos(\theta))$.

Gauss-Seidel: The matrix $T_\alpha$ satisfies property A after scaling by $\frac{1}{2}$, and this scaling term disappears once we form the iteration matrix. To find the eigenvalues of the scaled matrix, the discussion in the notes implies that the eigenvalues are either $0$ or squares of the singular values of the matrix with $-\frac{1}{2}$ on the diagonal and the subdiagonal. At the same time, the singular values of that matrix are the absolute values of the eigenvalues of the Jacobi iteration matrix, which are $\cos(j\theta)$. Thus, the greatest eigenvalue of the Gauss-Seidel iteration matrix is $\cos(\theta)^2$, which means that it converges because $\cos(\theta)<1$. In addition, this means that the rate of convergence is also squared, or $\log(\cos(\theta))^2$.

SOR converges for all $\omega\in(0,2)$ because $A$ is symmetric and positive definite.
\subsection*{2}
\ssn{a}
If $x^{(0)}=x$, then for all $k$, $x^{(k)}=x$, so $y^{(m)}$ can be written as $x\sum_{k=0}^m\alpha_k^{(m)}$. Since the $y^{(m)}$ are supposed to approximate the true solution $x$, it makes sense to constrain that sum to be $1$.
\ssn{b}
First, we note that $x^{(k)}=M^kx^{(0)}+\sum_{i=0}^{k-1}M^{i}b$. We have $x^{(0)}=e^{(0)}+x$, so substituting in gives $\sum_{i=0}^{k-1}M^{i}b+M^ke^{(0)}+M^kx$. Now, since $Mx=x-b$, we have $M^kx=x-\sum_{i=0}^{k-1}M^ib$, so substituting this back in gives $x^{(k)}=M^ke^{(0)}+x$, so $y^{(m)}=\sum_{k=0}^m\alpha_k^{(m)}(M^ke^{(0)}+x)=x+\sum_{k=0}^m\alpha_k^{(m)}M^ke^{(0)}$, and subtracting off the $x$ gives us $\ep^{(m)}$, which is now a polynomial of $M$ applied to $e^{(0)}$. The coefficients of the polynomial are the $\alpha^{(m)}$, which sum up to $1$.
\ssn{c}
Suppose that the limit isn't less than $1$. Then, for all $\ep>0$, there exists some arbitrarily large $m$ such that there is some associated $e_m^{(0)}$ of norm $1$ that satisfies $\|P_m(M)e_m^{(0)}\|_2\geq1-\ep$. Using the expression for $P_m(M)$ we found above, this is equivalent to 
\[\norm{\sum_{k=0}^m\alpha_k^{(m)}e_m^{(k)}}\geq1-\ep\implies\sum_{k=0}^m\alpha_k^{(m)}\|e_m^{(k)}\|\geq1-\ep\]
Let $m\to\infty$, and let $e^(0)$ be the limit of the $e_m^{(0)}$. We now suppose that the $e^{(k)}$ converge to zero. I can't find a good formal argument, but the idea is that if the norms of the $e^{(k)}$ drop off, then we can't get the sum above to be big enough since the $\alpha_k$ are constrained to sum to $1$, which results in a contradiction. 
\ssn{d}
In this case, $y^{(m)}=\frac{1}{m+1}\sum_{k=0}^mx^{(k)}$. Fix $\ep>0$. Then there exists some $N$ where $\|x^{(k)}-x\|<\ep$ for all $k>N$. Split the above sum into the $k\leq N$ and $k>N$ parts. As $m\to\infty$, the contribution from the $k\leq N$ part vanishes, and the other part becomes close to $x$ due to the convergence of the $x^{(k)}$.

For a counterexample, work in $\rn^1$ and let $x^{(k)}=(-1)^k$, which is clearly not convergent. Then $y^{(k)}$ is the harmonic sequence interspersed with zeros, which converges to zero. 
\subsection*{3}
\ssn{a}
The given polynomial evaluated at $M$ is zero, since $\det(xI-M)$ is the characteristic polynomial of $M$. The denominator is the characteristic polynomial evaluated at $1$, and $1$ cannot be a root since the spectral radius of $M$ is strictly less than $1$.
\ssn{b}
Unitarily diagonalize $M$ as $Q\Lambda Q^T$. Then, $\|P(M)\|=\|P(\Lambda)\|$, which is a diagonal matrix. Its $2$-norm is simply the maximum of the $P(\lambda_i)$, which is in turn bounded above by the sup norm of $P$ on $[\lambda_\text{min},\lambda_\text{max}]$, so we seek to minimize this over $P$ subject to the degree constraint and that $P(1)=1$. 
\ssn{c}
We know that the Chebyshev polynomials have sup norm $1$ on the interval $[-1,1]$, and since we're declaring that $P(1)=1$, this is the best we can do. Further, we have that the first two Chebyshev polynomials evaluated at $1$ are $1$, and by the recurrence relation given in the notes, we can induct on $k$ and show that all the polynomials come out to be $1$ at $1$. 
\ssn{d}
At $x=1$, the given expression is obviously $1$. For other values of $x$, the numerator is the Chebyshev polynomial shifted so that its argument lies between $[-1,1]$, so the numerator is minimized. The denominator is there for normalization.
\ssn{e}
Suppose another solution exists and call it $f$. By the properties of Chebyshev polynomials, all the extrema of $P_m(x)$ take the values $\pm\|P_m(x)\|_\infty$. 
\subsection*{4}
\ssn{a}
Taking the new values of $\lambda$, the polynomials $P_m(x)$ now take the form $\frac{C_m(x/\rho)}{C_m(1/\rho)}$. Using the recurrence relation from the notes, we have
\[C_{m+1}(1/\rho)P_{m+1}(x)=C_{m+1}(x/\rho)=\frac{2x}{\rho}C_m(x/\rho)-C_{m-1}(x/\rho)=\frac{2x}{\rho}C_m(1/\rho)P_m(x/\rho)-C_{m-1}(1/\rho)P_{m-1}(x)\]
\ssn{b}
Use the result from (2b), which states that $y^{(m)}-x=P_m(M)e_0$. Then, we can write 
\begin{align*}
    y^{(m+1)}-x=P_{m+1}(M)e_0&=\frac{2M}{\rho}\frac{C_m(1/\rho)}{C_{m+1}(1/\rho)}P_m(M)e_0-\frac{C_{m-1}(1/\rho)}{C_{m+1}(1/\rho)}P_{m-1}(M)e_0\\
                             &=\omega_{m+1}M(y^{(m)}-x)-\frac{C_{m-1}(1/\rho)}{C_{m+1}(1/\rho)}(y^{(m-1)}-x)\\
                             &=\omega_{m+1}(My^{(m)}-x-b)-\frac{C_{m-1}(1/\rho)}{C_{m+1}(1/\rho)}(y^{(m-1)}-x)\\
\end{align*}
Now note that 
\[\frac{C_{m-1}(1/\rho)}{C_{m+1}(1/\rho)}-\omega_{m+1}=\frac{\rho C_{m-1}(1/\rho)-2C_m(1/\rho)}{\rho C_{m+1}(1/\rho)}=-1\]
by the recurrence relation on the $C_m$, so we can use this to combine the $x$s and cancel out, so then we get 
\[y^{(m+1)}=\omega_{m+1}(My^{(m)}-b)-\frac{C_{m-1}(1/\rho)}{C_{m+1}(1/\rho)}y^{(m-1)}\]
Using the $-1$ identity from above again to expand the coefficient of $y^{(m-1)}$ gives us what we want.
\ssn{c}
Write $M=Q\Lambda Q^T$ so that $\|P_m(M)\|_2=\|P_m(\Lambda)\|_2$. This is the maximum value of $P_m(x)=\frac{C_m(x/\rho)}{C_m(1/\rho)}$ on the spectrum of $M$. The largest value that $C_m(x/\rho)$ can take anywhere on $[-\rho,\rho]$ is $1$, and it so happens that $\rho$ is an eigenvalue of $M$ and it attains the value $1$ there. Therefore, $\|P_m(M)\|_2=\frac{1}{C_m(1/\rho)}$. Now, since $1/\rho>1$, we have $C_m(1/\rho)=\cosh(m\cosh^{-1}(1/\rho))$. Since hyperbolic cosine is a monotonically increasing function for argument $>0$, $C_m(1/\rho)$ is monotonically increasing in $m$, so $\|P_m(M)\|_2$ is monotonically decreasing in $m$.
\ssn{d}
We have $\cosh^{-1}(x)=\log(x+\sqrt{x^2-1})$, so $\exp(\cosh^{-1}(1/\rho))=\frac{1}{\rho}(1+\sqrt(1-\rho^2))$. On the other hand, 
\[(\omega-1)^{-1}=\frac{1+\sqrt{1-\rho^2}}{1-\sqrt{1-\rho^2}}=\frac{(1+\sqrt{1-\rho^2})^2}{\rho^2}=\left(\frac{1+\sqrt{1-\rho^2}}{\rho}\right)^2=e^{2\sigma}\]
and the desired equality follows.

Then, we have
\[\cosh(m\sigma)=\frac{e^{-2m\sigma}+1}{2e^{-m\sigma}}=\frac{(\omega-1)^m+1}{2(\omega-1)^{m/2}}\]
and inverting this gives us $\|P_m(M)\|_2$.
\ssn{e}
This doesn't seem to be true. Take the polynomial $\lambda P_m(x)+(1-\lambda)$ for some $\lambda\in(0,1)$. Then the value of this at $1$ is $1$, it has degree $m$, and its sup norm is $\|\lambda P_m(x)+(1-\lambda)\|_\infty\leq\lambda\|P_m(x)\|_\infty+(1-\lambda)=1$.
\subsection*{5}
\ssn{a}
Here, $D$ is the identity, $L$ is $\openm0&0\\-M&0\closem$, and $U$ is $\openm0&-M\\0&0\closem$. Then, applying the SOR iteration gives
\[\openm x^{(k+1)}\\z^{(k+1)}\closem=\omega\left(\openm b\\b\closem-\openm0&0\\-M&0\closem\openm x^{(k+1)}\\z^{(k+1)}\closem-\openm0&-M\\0&0\closem\openm x^{(k)}\\z^{(k)}\closem\right)+(1-\omega)\openm x^{(k)}\\z^{(k)}\closem\]
Just looking at the top block gives us
\[x^{(k+1)}=\omega(b+Mz^{(k)})+(1-\omega)x^{(k)}=\omega(Mz^{(k)}-x^{(k)}+b)+x^{(k)}\]
as desired, and looking at the bottom block gives
\[z^{(k+1)}=\omega(b+x^{(k+1)})+(1-\omega)z^{(k)}=\omega(Mx^{(k+1)}-z^{(k)}+b)+z^{(k)}\]
\ssn{b}
Defining $y^{(m)}$ as given and using the above iterations, first assume $m=2k$. Then
\[y^{(m+1)}=z^{(k)}=\omega(Mx^{(k)}-z^{(k-1)}+b)+z^{(k-1)}=\omega(My^{(m)}-y^{(m-1)}+b)+y^{(m-1)}\]
and if $m=2k+1$, then
\[y^{(m+1)}=x^{(k+1)}=\omega(Mz^{(k)}-x^{(k)}+b)+x^{(k)}=\omega(My^{(m)}-y^{(m-1)}+b)+y^{(m-1)}\]
which is what we want.
\subsection*{6}
\ssn{a}
The gradient of a quadratic form $x^TAx$ is $A^Tx+Ax$, and the gradient of $b^Tx$ is $b$, so taken together the gradient of $\vp(x_k)$ is $A^Tx_k+Ax_k-2b=2Ax_k-2b=-2r_k$. Computing the second derivative, we have $2A$, which is positive definite. Therefore the stationary points minimize $\vp$.
\ssn{b}
If we want to minimize $\vp(x_k+\alpha p_k)$, we must have $\nabla\vp(x_k+\alpha p_k)=0$, or $2Ax_k+2\alpha Ap_k-2b=2\alpha Ap_k-2r_k=0$. Left-multiplying by $p_k^T$ gives $\alpha p_k^TAp_k-p_k^Tr_k$, or $\alpha=\frac{p_k^Tr_k}{p_k^TAp_k}$.
\ssn{c}
The expression for $\vp(x_{k+1})$ is the following. Subscripts are dropped because they're all subbed with $k$.
\[(x+\alpha p)^TA(x+\alpha p)-2b^T(x+\alpha p)=x^TAx+\alpha(p^TAx+x^TAp)+\alpha^2p^TAp-2b^Tx-2\alpha b^Tp\]
Note that $p^TAx=x^TAp$ because $A$ is symmetric. Subtracting off $\vp(x)=x^TAx-2b^Tx$ then gives
\[\frac{2(x^TAp)(r^Tp)}{p^TAp}+\frac{(r^Tp)^2}{p^TAp}-\frac{2(b^Tp)(r^Tp)}{p^TAp}\]
We have $r^T=b^T-x^TA$, so $x^TA=b^T-r^T$, and thus $x^TAp=b^Tp-r^Tp$. Then, expanding out the first term will cancel out the last term and subtract off twice the second term, so we get what we're looking for.
\ssn{d}
From the lectures, the iteration equation for steepest descent is $x^{(k+1)}=x^{(k)}+\alpha_kr^{(k)}$, which is the same as the one in (b) with $r^{(k)}$ in the place of $p_k$. The value of $\alpha_k$ is given in the lectures as $\frac{(r^{(k)T}r^{(k)}}{r^{(k)T}Ar^{(k)}}$, which is again the same as the $\alpha_k$ we derived in (b) with $r^{(k)}$ in the place of $p_k$.
\ssn{e}
$A$ is symmetric and positive definite, therefore orthogonally diagonalizable as $Q\Lambda Q^T$. Then 
\[\|P(A)x\|_A^2=x^TP(A)Q\Lambda Q^TP(A)x=x^TQP(\Lambda)Q^TQ\Lambda Q^TQP(\Lambda)Q^Tx=x^TQP(\Lambda)^2\Lambda Q^Tx=x^TP(\Lambda)^2Ax\]
We can bound this from above by replacing $P(\Lambda)$ with a matrix with $\max(P(\lambda_i))^2$ on the diagonals, thereby getting the upper bound $\max(P(\lambda_i))\|x\|_A^2$. Taking the square root on both sides gives the desired inequality.
\ssn{f}
We have $P_\alpha(A)(x_{k-1}-x_*)=x_{k-1}-x_*-\alpha Ax_{k-1}-\alpha Ax_*=x_{k-1}-x_*-\alpha Ax_{k-1}+\alpha b=x_{k-1}-x_*+\alpha r_{k-1}=x_k-x_*$, so the inequality follows by plugging into the bound in (e).
\ssn{g}
The given polynomial takes the value $0$ at $1/\alpha$. Since it's a line with a bend in it, the maximum value of $|1-\alpha t|$ must be attained at one of the endpoints, $t=\lambda_n$ or $t=\lambda_1$. For $1/\alpha<\frac{\lambda_1+\lambda_n}{2}\equiv \mu$, the maximum occurs at $t=\lambda_n$ and otherwise at $t=\lambda_1$, since the graph of $|1-\alpha t|$ is symmetric about $1/\alpha$. Then, the maximum as a function of $\alpha$ can be expressed as
\[
    g(\alpha)=\begin{cases}
        |1-\alpha\lambda_n|&\text{if }\alpha\geq\frac{1}{\mu}\\
        |1-\alpha\lambda_1|&\text{if }\alpha<\frac{1}{\mu}\\
    \end{cases}
\]
The minimum occurs precisely when $\alpha=\frac{1}{\mu}$, which gives the maximum value as $1-\frac{2\lambda_n}{\lambda_1+\lambda_n}=\frac{\lambda_1-\lambda_n}{\lambda_1+\lambda_n}$. Then, plugging this into the bound from (f) gives us what we want.
\subsection*{7}
GEPP, Cholesky, and steepest descent were coded up with vectorized operations, and Gauss-Seidel was coded up in a fast, non-interpreted language (Cython). Since there are so many hardware optimizations for vectorized code out there, the benefits of vectorization should be considered when choosing an algorithm. The code was tested on a range of densities varying from $0.001$ to $0.5$, and on sizes ranging from $10$ to $500$. Throughout, the $b$ vector used was sampled from a standard normal distribution, and the nonzero entries of $A$ are uniformly generated between $0$ and $1$. 

The GEPP and Cholesky algorithms were coded to work on densified versions of these matrices, since the sparse implementation of these were incredibly slow due to the need to dynamically allocate memory to store new nonzero elements.

Accuracy was measured by calculating $\|A\h{x}-b\|_2$. Since we can control the accuracy of the two iterative methods by adjusting the criterion for convergence, we let the iterative algorithms terminate when they surpass the accuracy of the non-iterative algorithm on the corresponding matrix (GEPP vs. Gauss-Seidel and Cholesky vs. steepest descent). 

Throughout, in the graphs presented, GEPP is represented with a dot, Cholesky is represented with a star, steepest descent is represented with a circle, and Gauss-Seidel is represented with a plus.

First, we analyze the effect of shape and density on runtime. Below is a plot of runtime versus size for density-$0.02$

\includegraphics[width=0.6\textwidth]{hw5_files/runtime_shape_20.png}

On a log-log plot, it seems that the runtime for the non-iterative methods increases faster than that for the iterative methods, and the runtimes for the methods on symmetric, positive-definite matrices are overall lower than those for general matrices.

For the density-runtime relation, we choose the shape to be $200$.

\includegraphics[width=0.6\textwidth]{hw5_files/runtime_density_200.png}

As expected, the runtimes for the non-iterative methods do not change, as they're operating on the dense version of those matrices. Gauss-Seidel behaves as expected and increases in runtime for increased density, but steepest descent levels off after a while. This is probably due to the larger matrices incurring greater error, and therefore steepest descent needs less iterations to finish.

Now, we look at the effects of size and density on the number of iterations. First we look at iterations versus size for density $0.05$.

\includegraphics[width=0.6\textwidth]{hw5_files/iters_shape_50.png}

The size doesn't seem to have much effect past a certain point, with the number of iterations stabilizing. Note that all the iteration counts for Gauss-Seidel are a multiple of 10. This is due to the implementation only checking for error every tenth iteration to save on time.

Iterations versus density for size $300$:

\includegraphics[width=0.6\textwidth]{hw5_files/iters_density_300.png}

Curiously, the number of iterations needed for steepest descent rises, then drops with increasing density, and the number of iterations for Gauss-Seidel remains constant. This is probably related to the trend in accuracy with density, as we will see next.

Backwards error versus size for density $0.01$ is next. We only plot the backwards error for the non-iterative methods, as the accuracy of the iterative methods is pinned to these.

\includegraphics[width=0.6\textwidth]{hw5_files/acc_shape_10.png}

Error grows steadily with size, as expected. For some reason, the error from the Cholesky method is consistently greater than that from GEPP, but not by much. 

Backwards error versus density for size $400$:

\includegraphics[width=0.6\textwidth]{hw5_files/acc_density_400.png}

Surprisingly, density actually has an effect on error. I guess it's because all the zeros in the more sparse matrices don't incur rounding errors when getting multiplied or added.

Finally, we remark that since the runtime for the iterative methods to achieve the same amount of error as the non-iterative methods is smaller, we can probably say that if they ran for the same amount of time, then the iterative methods would incur less error. The main disadvantage then with the iterative methods is that they do not allow for a quick solution with another value of $b$.
\end{document}
