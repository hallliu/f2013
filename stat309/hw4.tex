\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\im}{im}
\DeclareMathOperator{\Span}{span}
\DeclareMathOperator{\Null}{null}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\cn}{\mathbb{C}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\nc{\h}[1]{\widehat{#1}}
\nc{\tl}[1]{\widetilde{#1}}
\nc{\norm}[1]{\left\|{#1}\right\|}
\begin{document}

Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
\ssn{a}
Suppose $U$ were singular. Then, its image would have dimension $<n$, which means that the image of $LU$ would have dimension $<n$. However, since $A$ is full rank, the image of $\Pi_1A\Pi_2$ must have dimension $n$, contradiction. We also know that $L_1$ is nonsingular because it is lower triangular and all of its diagonal entries are $1$s, giving it a determinant of $1$.
\ssn{b}
Starting with $\|Ax-b\|$, left-multiply by $\Pi_1$ to obtain $\|\Pi_1Ax-\tl{b}\|$. Note that since $LU=\Pi_1A\Pi_2$, we have $\Pi_1A=LU\Pi_2^T$, so the quantity to be minimized is then $\|LU\tl{x}-\tl{b}\|$. Let $y=U\tl{x}$. Then, we want to minimize $\|Ly-\tl{b}\|$, which we can do by solving the normal equation $L^TLy=L^Tb$. We can then find $\tl{x}$ by backsolving $y=U\tl{x}$ (where a unique solution is guaranteed because $U$ is invertible).

We can solve the normal equation quickly by backsolving twice: let $\tl{y}=Ly$, and backsolve $L^T\tl{y}=L^Tb$ for $\tl{y}$. Then backsolve $\tl{y}=Ly$ for $y$.
\subsection*{2}
\ssn{a}
If we form $A^TA$, we obtain $\openm3&3\\3&3+2\ep^2\closem$, which is really, really close to being singular if $\ep$ is small. In addition, $\ep^2$ may fall below machine precision (at least compared to the other entries), which would actually make the matrix singular.
\ssn{b}
Multiply them together, get $A$. $L$ and $U$ are easily recognized to be in the correct form.
\ssn{c}
The $L$ matrix found has no $\ep$ terms: none of its terms have small perturbations, and the columns are much further away from being linearly dependent than the columns of $A$. Thus, it's a better conditioned problem than solving the normal equation directly.
\ssn{d, e}
Since the rank of $A$ is 2, the $LU$ factorization above is a rank-retaining factorization. Thus, from the formula we have for pseudoinverse from a rank-retaining factorization, we have $A^+=U^T(UU^T)^{-1}(L^TL)^{-1}L^T$. Computing each of them, we have $UU^T=\openm2&\ep\\\ep&\ep^2\closem$, so its inverse is $\openm1&-\ep^{-1}\\-\ep^{-1}&2\ep^{-2}\closem$. $L^TL=\openm3&0\\0&2\closem$ so $(L^TL)^{-1}=\openm1/3&0\\0&1/2\closem$. Forming the big product, 
\begin{align*}
    A^+&=\openm1&0\\1&\ep\closem\openm1&-\ep^{-1}\\-\ep^{-1}&2\ep^{-2}\closem\openm1/3&0\\0&1/2\closem\openm1&1&1\\0&1&-1\closem\\
       &=\openm1&-\ep^{-1}\\0&\ep^{-1}\closem\openm1/3&0\\0&1/2\closem\openm1&1&1\\0&1&-1\closem\\
       &=\openm1/3&-\ep^{-1}/2\\0&\ep^{-1}/2\closem\openm1&1&1\\0&1&-1\closem\\
       &=\openm1/3&1/3-\ep^{-1}/2&1/3+\ep^{-1}/2\\0&\ep^{-1}/2&-\ep^{-1}/2\closem\\
\end{align*}
which gives us the expression desired after factoring out a $\frac{1}{6}$.
\ssn{3}
\ssn{a}
Using what we had in (1a), the least squares problem is equivalent to minimizing $\|LU\tl{x}-\tl{b}\|=\|Ly-\tl{b}\|$. If we write $L=\openm L_1\\L_2\closem$, then $Ly=\openm L_1y\\L_2y\closem=\openm z\\L_2L_1^{-1}z\closem=\openm I_n\\S\closem z$, so the original problem is then equivalent to $\norm{\openm I_n\\S\closem z-\tl{b}}$. 
\ssn{b}
Note that the given solution for $z$ corresponds to $\openm I_n-S^T(I_{m-n}+SS^T)^{-1}S&S^T(I_{m-n}+SS^T)^{-1}\closem b$. Denote this matrix by $K$. Now, note that since the matrix $A=\openm I_n\\S\closem$ is full rank, its pseudoinverse is $(A^TA)^{-1}A^T=(I_n-S^TS)^{-1}\openm I_n&S^T\closem=\openm(I_n+S^TS)^{-1}&(I_n-S^TS)^{-1}S^T\closem$.

We want to show that $A^+=K$. Go block by block. We want to show that $I_n-S^T(I_{m-n}+SS^T)^{-1}S=(I_n-S^TS)^{-1}$, or that $(I_n-S^TS)(I_n-S^T(I_{m-n}+SS^T)^{-1}S)=I_n$. Multiplying it out, we have
\[I_n+S^TS-S^T(I_{m-n}+SS^T)^{-1}S-S^TSS^T(I_{m-n}+SS^T)^{-1}S=I_n+S^TS-S^T(I_{m-n}+SS^T)(I_{m-n}+SS^T)^{-1}S=I_n\]
For the second block, we want to show that $(I_n+S^TS)^{-1}S^T=S^T(I_{m-n}+SS^T)^{-1}$. Multiplying out the inverses, we get $S^T(I_{m-n}+SS^T)=(I_n+S^TS)S^T$, or $S^T+S^TSS^T=S^T+S^TSS^T$, which is true.

Since $z$ is given by $A^+b$, we know that $z$ must be the min-norm least squares solution.
\ssn{c}
Let $\tl{z}=z-\tl{b}_1$, and let $c=\tl{b}_2-S\tl{b}_1$. Then, we want to obtain $\tl{z}=S^T(I_{m-n}+SS^T)^{-1}c$. $S^T$ is a $(m-n)\times n$ matrix, and $(I_{m-n}+SS^T)$ is a $(m-n)\times(m-n)$ matrix. If $m-n$ is small, then these matrices will be small, and consequently it will be easier to obtain $\tl{z}$.
\subsection*{4}
\ssn{a}
We have $\Pi_1A\Pi_2=LU$, so transposing both sides gives $\Pi_2^TA^T\Pi_1^T=U^TL^T$. If we let $\tl{w}=\Pi_1w$, then $A^Tw=c$ is equivalent to $A^T\Pi_1^T\tl{w}=c$. Left-multiplying by $\Pi_2^T$ on both sides gives that this is equivalent to $U^TL^T\tl{w}=\tl{c}$, since $\Pi_2$ is bijective.

Minimizing $\norm{\tl{w}}$ is equivalent to minimizing $\|w\|$ due to $\Pi_1$ being orthogonal. We are guaranteed that some $\tl{w}$ exists that satisfies this constraint, since $A^T$ has rank $n$ and $\tl{c}\in\rn^n$. Thus, the minimization problem is equivalent to solving the min-norm least squares problem of $\norm{U^TL^T\tl{w}-\tl{c}}$. 

To solve this, take the pseudoinverse of $U^TL^T$. Since this is already in rank-retaining form, we use the formula for pseudoinverse given in the lecture notes, $L(L^TL)^{-1}(UU^T)^{-1}U=L(L^TL)^{-1}U^{-T}U^{-1}U=L(L^TL)^{-1}U^{-T}$ since $U$ is invertible. Thus we have that the solution is given by $\tl{w}=L(L^TL)U^{-T}\tl{c}$.
\ssn{b}
Write the above equation as $\openm\tl{w}_1\\\tl{w}_2\closem=\openm L_1\\L_2\closem(L^TL)^{-1}U^{-T}\tl{c}$. Then, we have $\tl{w}_1=L_1(L^TL)^{-1}U^{-T}\tl{c}$ and $\tl{w}_2=L_2(L^TL)^{-1}U^{-T}\tl{c}$. Since $L_1$ is invertible, we can express $\tl{c}$ in terms of $\tl{w}_1$ as $\tl{c}=U^T(L^TL)L_1^{-1}\tl{w}_1$. Substituting this into the second equation, we have $\tl{w}_2=L_2(L^TL)^{-1}U^{-T}U^T(L^TL)L_1^{-1}\tl{w}_1=S\tl{w}_1$. 

Consider $\tl{c}$ again. We have 
\begin{align*}
    \tl{c}&=U^T(L_1^TL_1+L_2^TL_2)L_1^{-1}\tl{w}_1\\
    \tl{c}&=U^TL_1^T\tl{w}_1+U^TL_2^TS\tl{w}_1\\
    L_1^{-T}U^{-T}\tl{c}&=\tl{w}_1+L_1^{-T}L_2^T\tl{w}_2\\
    L_1^{-T}U^{-T}\tl{c}-S^T\tl{w}_2&=\tl{w}_1\\
\end{align*}
which is what we wanted.
\ssn{c}
If we expand out the thing inside the norm, we get $\openm S^T\tl{w}_2-d\\\tl{w}_2\closem$, which is the same thing as $\openm -\tl{w}_1\\\tl{w}_2\closem$. The norm of this is the same as the norm of $\tl{w}$, so minimizing one is equivalent to minimizing the other.

Now, as before, the solution is given by $\tl{w}_2=\openm S^T\\I_{m-n}\closem^+\openm d\\0\closem$. Since the matrix $A=\openm S^T\\I_{m-n}\closem$ we're taking the pseudoinverse of is already full column rank, its pseudoinverse is given by
\[(A^TA)^{-1}A^T=(SS^T+I_{m-n})^{-1}\openm S&I_{m-n}\closem\]
Applying this to $\openm d\\0\closem$, we get $(SS^T+I_{m-n})^{-1}Sd$, as desired.
\subsection*{5}
\ssn{a}
Due to $L$ being unit lower triangular, $L_1$ is nonsingular because none of its diagonal entries are zero. Further, the entries on the diagonal of $U_1$ come from the pivoted diagonal entries of the successive iterations of $A^{(i)}$. Since each $A^{(i)}$ has the same rank as $A$, failing to find $r$ nonzero entries would imply that some $A^{(i)}$ has a block of zeros in the bottom larger than $m-r$, contradicting the same-rank-ness.
\ssn{b}
We have $LU=\openm L_1\\L_2\closem\openm U_1&U_2\closem=\openm L_1U_1&L_1U_2\\L_2U_1&L_2U_2\closem$, and
\[\openm I_r\\S_1\closem L_1U_1\openm I_r&S_2^T\closem=\openm L_1U_1\\S_1L_1U_1\closem\openm I_r&S_2^T\closem=\openm L_1U_1&L_1U_1S_2^T\\S_1L_1U_1&S_1L_1U_1S_2^T\closem\]
We must have $U_1S_2^T=U_2$, or $S_2^T=U_1^{-1}U_2$. Additionally, we must have $S_1L_1=L_2$, or $S_1=L_2L_1^{-1}$. Plugging these in makes it work.
\ssn{c}
We have that $\openm I_r&S_2^T\closem$ is full-row-rank because of the identity in the beginning and $\openm I_r\\S_1\closem$ is full-col-rank for the same reason. Now, since $U_1$ and $L_1$ are both full rank because invertible, we have 
\[\left(\openm I_r\\S_1\closem L_1U_1\openm I_r&S_2^T\closem\right)^+=\openm I_r&S_2^T\closem^+\left(\openm I_r\\S_1\closem L_1U_1\right)^+=\openm I_r&S_2^T\closem^+U_1^{-1}\left(\openm I_r\\S_1\closem L_1\right)^+=\openm I_r&S_2^T\closem^+U_1^{-1}L_1^{-1}\openm I_r\\S_1\closem^+\]
This is the pseudoinverse of $\Pi_1A\Pi_2$, or $\Pi_2^TA^+\Pi_1^T$. Moving the permutation matrices to the other side gives us what we want.
\ssn{d}
The $LU$ given in part (a) is a rank-retaining factorization for $\Pi_1A\Pi_2$, so its pseudoinverse is $U^T(UU^T)^{-1}(L^TL)^{-1}L^T$
\subsection*{6}


\end{document}
