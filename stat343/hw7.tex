\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\im}{im}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\h}[1]{\widehat{#1}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\begin{document}
Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{6.3}
We fit the desired combination of variables by creating a new data set, \verb|sal1|, and assigning its \verb|dose| field to be the log of \verb|salmonella$dose + 1|. The results are as follows:
\begin{verbatim}
Call:
lm(formula = colonies ~ dose, data = sal1)

Residuals:
    Min      1Q  Median      3Q     Max 
-16.376  -6.882  -1.509   5.400  29.119 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)   19.823      5.064   3.915  0.00123 **
dose           2.396      1.128   2.125  0.04955 * 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 10.84 on 16 degrees of freedom
Multiple R-squared:  0.2201,    Adjusted R-squared:  0.1713 
F-statistic: 4.514 on 1 and 16 DF,  p-value: 0.04955
\end{verbatim}
and the plot is

\includegraphics[width=0.6\textwidth]{hw7_files/6_3_plot1.png}

As seen from the low $R^2$ value and the poor appearance of the plot, it may be a good idea to check for lack of fit. If we do so by fitting a model with parameters for each level of dose with
\begin{verbatim}
> a_alt = lm(colonies ~ factor(dose), sal1)
> points(sal1$dose, fitted(a_alt), pch=18)
> anova(a, a_alt)
Analysis of Variance Table

Model 1: colonies ~ dose
Model 2: colonies ~ factor(dose)
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     16 1881.1                           
2     12 1091.3  4    789.73 2.1709 0.1342
\end{verbatim}
Since the $p$-value is high enough, we should accept the null hypothesis that the fit is good. Instead, the low $R^2$ value can be attributed to the naturally high variance in the observations.
\subsection*{6.5}
Least squares:
\begin{verbatim}
> a = lm(stack.loss ~ ., data=stackloss)
> summary(a)

Call:
lm(formula = stack.loss ~ ., data = stackloss)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.2377 -1.7117 -0.4551  2.3614  5.6978 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -39.9197    11.8960  -3.356  0.00375 ** 
Air.Flow      0.7156     0.1349   5.307  5.8e-05 ***
Water.Temp    1.2953     0.3680   3.520  0.00263 ** 
Acid.Conc.   -0.1521     0.1563  -0.973  0.34405    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 3.243 on 17 degrees of freedom
Multiple R-squared:  0.9136,    Adjusted R-squared:  0.8983 
F-statistic:  59.9 on 3 and 17 DF,  p-value: 3.016e-09
\end{verbatim}
LAD:
\begin{verbatim}
> b = rq(stack.loss ~ ., data=stackloss)
> summary(b)

Call: rq(formula = stack.loss ~ ., data = stackloss)

tau: [1] 0.5

Coefficients:
             coefficients lower bd  upper bd 
(Intercept) -39.68986    -41.61973 -29.67754
Air.Flow      0.83188      0.51278   1.14117
Water.Temp    0.57391      0.32182   1.41090
Acid.Conc.   -0.06087     -0.21348  -0.02891
\end{verbatim}
Huber's:
\begin{verbatim}
> c = rlm(stack.loss ~ ., data=stackloss)
> summary(c)

Call: rlm(formula = stack.loss ~ ., data = stackloss)
Residuals:
Min       1Q   Median       3Q      Max 
-8.91753 -1.73127  0.06187  1.54306  6.50163 

Coefficients:
               Value    Std. Error t value 
(Intercept) -41.0265   9.8073    -4.1832
Air.Flow      0.8294   0.1112     7.4597
Water.Temp    0.9261   0.3034     3.0524
Acid.Conc.   -0.1278   0.1289    -0.9922

Residual standard error: 2.441 on 17 degrees of freedom
\end{verbatim}
Trimmed least squares:
\begin{verbatim}
> d = ltsreg(stack.loss ~ ., data=stackloss, nsamp="exact")
> d
Call:
lqs.formula(formula = stack.loss ~ ., data = stackloss, nsamp = "exact", 
    method = "lts")

Coefficients:
(Intercept)     Air.Flow   Water.Temp   Acid.Conc.  
 -3.581e+01    7.500e-01    3.333e-01    3.489e-17  

Scale estimates 0.8482 0.8645 
\end{verbatim}
The water temperature coefficient is very high for the least-squares method compared to the LAD or the trimmed least squares model, and it's somewhere in the middle for Huber's method. This suggests that there are outliers that are affecting least squares but not the more robust models. We see a similary magnitude difference with acid concentration, which suggests that it would be good to take a look at that too. Acid concentration goes from being insignificant under least squares to significant under LAD. Let's take a look at the diagnostics.

First, take a look at the jackknife residuals. We see that case $21$ has a jackknife residual of $-3.33$, while the Bonferroni threshold is $3.60$. This is pretty close, and if we take a look at the plot of stack loss versus airflow, we have the following:

\includegraphics[width=0.6\textwidth]{hw7_files/6_5_airflow.png}

Case 21 looks like it's quite a bit below the line. Let's remove it and see what least squares gives back:
\begin{verbatim}
> a1=lm(stack.loss ~ ., data=stackloss1)
> summary(a1)

Call:
lm(formula = stack.loss ~ ., data = stackloss1)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.0449 -2.0578  0.1025  1.0709  6.3017 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -43.7040     9.4916  -4.605 0.000293 ***
Air.Flow      0.8891     0.1188   7.481 1.31e-06 ***
Water.Temp    0.8166     0.3250   2.512 0.023088 *  
Acid.Conc.   -0.1071     0.1245  -0.860 0.402338    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 2.569 on 16 degrees of freedom
(1 observation deleted due to missingness)
Multiple R-squared:  0.9488,  Adjusted R-squared:  0.9392 
F-statistic: 98.82 on 3 and 16 DF,  p-value: 1.541e-10
\end{verbatim}
This produces something more in line with what we got back from the more robust models.
\subsection*{7.2}
First, examine the raw scatterplot below. It does not seem to be a linear trend, nor does it seem to be piecewise or polynomial. Thus, transforming the response is probably our best bet, and we do so with the Box-Cox method.

\includegraphics[width=0.6\textwidth]{hw7_files/7_2_raw.png}

Running Box-Cox on the data gives us the following plot 

\includegraphics[width=0.6\textwidth]{hw7_files/7_2_boxcox.png}

which suggests that we should try transforming because of the apparent curvature. Using $\lambda=3$, we transform the response and get the following scatterplot:

\includegraphics[width=0.6\textwidth]{hw7_files/7_2_transformed.png}

This looks more reasonable, so we fit a least-squares model to it and receive the following output

\begin{verbatim}
> n = (cornnit$nitrogen^(1/3)-1)/3
> a2 = lm(yield ~ n, data=cornnit)
> summary(a2)
Call:
lm(formula = yield ~ n, data = cornnit)

Residuals:
    Min      1Q  Median      3Q     Max 
-44.681  -6.977   3.225  12.000  23.319 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  100.821      3.515  28.685  < 2e-16 ***
n             27.421      2.944   9.313 8.98e-12 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 15.09 on 42 degrees of freedom
Multiple R-squared:  0.6737,    Adjusted R-squared:  0.666 
F-statistic: 86.73 on 1 and 42 DF,  p-value: 8.984e-12
\end{verbatim}

The $R^2$ value didn't increase much, but the plot displays a lot of variability for the response to each value of the predictor, so let's run a goodness-of-fit test on it. Creating a factor model with \verb|a1 = lm(yield ~ factor(nitrogen), data=cornnit)| and running ANOVA with \verb|a2| against \verb|a1|, we get
\begin{verbatim}
Analysis of Variance Table

Model 1: yield ~ n
Model 2: yield ~ factor(nitrogen)
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     42 9563.7                           
2     37 8186.8  5    1376.9 1.2446 0.3083
\end{verbatim}

which suggests that this is a good fit. If we look at the fitted line overlaid on the scatterplot below, it would seem to be a reasonable conclusion.

\includegraphics[width=0.6\textwidth]{hw7_files/7_2_t_fitted.png}
\subsection*{7.3}
Fitting a linear model, we get 
\begin{verbatim}
Call:
lm(formula = O3 ~ temp + ibh + humidity, data = ozone)

Residuals:
     Min       1Q   Median       3Q      Max 
-11.5291  -3.0137  -0.2249   2.8239  13.9303 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.049e+01  1.616e+00  -6.492 3.16e-10 ***
temp         3.296e-01  2.109e-02  15.626  < 2e-16 ***
ibh         -1.004e-03  1.639e-04  -6.130 2.54e-09 ***
humidity     7.738e-02  1.339e-02   5.777 1.77e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 4.524 on 326 degrees of freedom
Multiple R-squared:  0.684,  Adjusted R-squared:  0.6811 
F-statistic: 235.2 on 3 and 326 DF,  p-value: < 2.2e-16
\end{verbatim}
If we run the Box-Cox method, we get the following plot of log-likelihood, which suggests that we should use $\lambda=0.25$(corresponding to a quartic root for physical interpretability). 

\includegraphics[width=0.6\textwidth]{hw7_files/7_3_boxcox.png}

Then, if we apply the transformation and re-fit the model, we get 

\begin{verbatim}
Call:
lm(formula = I((O3^(1/4) + 1) * 4) ~ temp + ibh + humidity, data = ozone)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.17305 -0.43348  0.03639  0.49897  1.97315 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)  7.655e+00  2.474e-01  30.945  < 2e-16 ***
temp         5.227e-02  3.228e-03  16.192  < 2e-16 ***
ibh         -1.939e-04  2.508e-05  -7.734 1.31e-13 ***
humidity     1.191e-02  2.050e-03   5.810 1.48e-08 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.6923 on 326 degrees of freedom
Multiple R-squared:  0.7152, Adjusted R-squared:  0.7126 
F-statistic: 272.9 on 3 and 326 DF,  p-value: < 2.2e-16
\end{verbatim}
This suggests that it was a good transformation to make -- the $R^2$ value increased and the predictors became more significant.
\subsection*{6.15}
\ssn{1}
\includegraphics[width=0.6\textwidth]{hw7_files/6_15_plot1.png}

A linear fit seems like it wouldn't work too well here -- there seems to be a distinct downwards curve in the trend rather than a straight line.
\ssn{2}
If we run a goodness of fit on the linear model and plot the results, we get 

\includegraphics[width=0.6\textwidth]{hw7_files/6_15_linear_fit.png}

where the black dots are the level means and the line is the fitted line. The $R^2$ value for this fit was $0.78$. The $F$-test gives a $p$-value of $1.062\times10^{-7}$, indicating that this is not a good fit.

If we use a quadratic model, we get the following

\includegraphics[width=0.6\textwidth]{hw7_files/6_15_quadratic_fit.png}

\begin{verbatim}
Call:
lm(formula = Gain ~ poly(A, 2), data = turk0)

Residuals:
    Min      1Q  Median      3Q     Max 
-32.988 -16.542   2.193  12.788  36.059 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  705.914      3.182 221.849  < 2e-16 ***
poly(A, 2)1  353.113     18.825  18.758  < 2e-16 ***
poly(A, 2)2 -154.390     18.825  -8.201 2.28e-09 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 18.82 on 32 degrees of freedom
Multiple R-squared:  0.9291,    Adjusted R-squared:  0.9246 
F-statistic: 209.6 on 2 and 32 DF,  p-value: < 2.2e-16
\end{verbatim}
All of the predictors are significant, and the $R^2$ value is dramatically higher, which suggests that the quadratic model is a superior one. If we test for lack-of-fit on this, the reduction in $RSS$ is enough to make the $p$-value $0.2374$, which suggests that there is no significant lack of fit for the quadratic model.
\ssn{3}
The quadratic mean function seems to conform to the trend in the level means much better than the linear fit -- the downwards curve in the data is accounted for in the quadratic fit.
\subsection*{6.16}
For this problem, we will use the standard polynomial basis, since it's a lot harder to differentiate in the orthogonal basis than the standard. If we assume that the fitted quadratic mean function is $\h{f}(x)=\h{\beta}_0+\h{\beta}_1x+\h{\beta}_2x^2$, then the fitted function is maximized at $x=-\frac{\h{\beta}_1}{2\h{\beta}_2}$. Thus, we can bootstrap using the following code:

\begin{verbatim}
maxima = numeric(1000)
for (i in 1:1000) {
    sample_data = turk0[sample(35, replace=TRUE),]
    coefs = coef(lm(Gain ~ A + A^2, data=sample_data))
    maxima[i] = -coefs[2]/(2*coefs[3])
}
quantile(maxima, c(0.025, 0.975))
\end{verbatim}

This gives us a $95\%$ confidence interval of $(0.328, 0.401)$ with an estimate of $0.355$ for optimal percentage of methionine supplement, which corresponds to a standard error of $0.0183$. 

Using the delta method, the variance is given by (6.31) in Weisberg, so plugging that into R and computing gives $0.00444$, which is considerably lower than that obtained from bootstraping.


\end{document}
