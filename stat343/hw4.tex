\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\rk}{rk}
\DeclareMathOperator{\im}{im}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\h}[1]{\widehat{#1}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\inner}[2]{\langle #1,#2\rangle}
\begin{document}
Name: Hall Liu

Date: \today 
\vspace{1.5cm}
\subsection*{1}
Let $h_1,\ldots,h_r$ be an o.n. basis for $H$, so $Qh_i$ for $i\in[1,r]$ is an o.n. basis for $QH$. Since $Q$ is bijective, each vector in $QH^\perp$ can be expressed as $Qv$ for some $v\in H^\perp$. Thus, let $Qv\in QH\perp$. Taking the inner product with all the basis elements of $QH$, we have $\inner{Qv}{Qh_i}=\inner{v}{h_i}=0$, so $Qv\in(QH)^\perp$. This implies that $QH^\perp\subset(QH)^\perp$. Now, since $Q$ is bijective, $\dim(QH)=\dim(H)=r\implies\dim((QH)^\perp)=n-r$ and $\dim(H^\perp)=\dim(QH^\perp)=n-r$, which implies equality.
\subsection*{2}
Since $H$ is idempotent, we have $H=H^2$, which implies that $h_{ii}=\sum_jh_{ij}h_{ji}=\sum_jh_{ij}^2$, since $H$ is symmetric. Then, since $HX=X$, multiplying $H$ by the first column of $X$ (the one with all $1$s) should give all $1$s back. Thus, for any $i$, $\sum_j1\cdot h_{ij}=1$, or $\sum_j h_{ij}=1$. Now, denote the $i$th row/column of $H$ by $h_i$. By Cauchy-Schwarz, $1=\inner{h_i}{1}^2\leq\inner{h_i}{h_i}\inner{1}{1}=nh_{ii}$, so we have $h_{ii}\geq\frac{1}{n}$.

For the other inequality, note that if $x_i=x_j$, then $h_{ii}=h_{ij}$ since $h_{ij}=x_i(X^TX)^{-1}x_j^T$. Then, we have $h_{ii}=rh_{ii}^2+\sum_{x_i\neq x_j}h_{ij}^2\leq rh_{ii}^2$. Dividing on both sides by $h_{ii}$ (which is strictly positive because $X^TX$ is pos. def.), we have $1\leq rh_{ii}$, or $h_{ii}\leq\frac{1}{r}$.

Let $X=\openm1&0\\1&1\closem$. Then $H$ is the identity, which achieves equality for the upper bound. 
For the lower bound, take the model with only the intercept, so let $X=\openm1&1&\hdots&1\closem^T$. Then we have $X(X^TX)^{-1}X^T$ is the matrix filled with values $1/n$, which attains the lower bound.
\subsection*{3}
\ssn{a}
\noindent\includegraphics[width=0.5\textwidth]{hw4_files/3a_resids.png}
\includegraphics[width=0.5\textwidth]{hw4_files/3a_null.png}

There appears to be no non-linearity or non-constant variance present. Comparing to the null plot to the right, there is no visible difference here.

\noindent\includegraphics[width=0.5\textwidth]{hw4_files/3a_lcavol_resids.png}

Looking at the plot of the most significant predictor (log of cancer volume) versus residuals, we see pretty much the same thing: no trend.
\ssn{b}
We can take the Q-Q plot of the residuals and compare them to a ``null plot'' generated from a random normal sample, as seen in the two plots here.

\noindent\includegraphics[width=0.5\textwidth]{hw4_files/3b_resids.png}
\includegraphics[width=0.5\textwidth]{hw4_files/3b_baseline.png}

They look very similar, which suggests that the normality assumption on the residuals is sound.
\ssn{c}
We now compare the half-normal plot of the leverages with a null plot. Case 32 is the most extreme of the leverages, with a value of $0.3304$. Since twice the ``average'' leverage should be $2\cdot9/97=0.186$, this point is probably worth taking a look at.

\noindent\includegraphics[width=0.5\textwidth]{hw4_files/3c_leverages.png}
\includegraphics[width=0.5\textwidth]{hw4_files/3c_null.png}

\ssn{d}
We can check outliers by using the jackknife residuals, found with the R command \verb|rstudent()|. Running this gives case 39 as the one with the most extreme jackknife residual, with a value of $-2.217$. This is surprising because this point actually had fairly low leverage. It could be that this point was only distant in one dimension from the rest of the data, which means that it actually had fairly low pull on the regression parameters. 

Now, we can calculate the Bonferroni threshold with \verb|qt(0.05/(97*2), 87)|, and this gives $-3.607$, which indicates that case 39 isn't really much of an outlier at all.
\ssn{e}
Checking the Cook distance on a half-normal plot, we see that 32, 47, 69, and 95 are the cases that stand out the most. Taking the most outstanding one, 32, and computing a new model \verb|c| with it excluded, we can find the relative difference in the results
\begin{verbatim}
 (Intercept)       lcavol      lweight          age         lbph          svi          lcp      gleason 
 0.743234304  0.036947598 -0.367892871 -0.083224201  0.107082175  0.007484213 -0.004861717 -0.122875708 
       pgg45 
 0.012578993
\end{verbatim}
Thus, we see that there was a very large change from excluding point 32.
\ssn{f}
We construct a partial regression plot for log of cancer volume with the following commands:
\begin{verbatim}
> no_lcavol = lm(lpsa ~ . - lcavol, data=prostate)
> lcavol_others = lm(lcavol ~ . - lpsa, data=prostate)
> lpsa_resids = residuals(no_lcavol)
> lcavol_resids = residuals(lcavol_others)
> plot(lcavol_resids, lpsa_resids)
\end{verbatim}

The resulting plot shows fairly good linearity without any immediately obvious outliers, which leads us to believe that there isn't a problem with the effect of lcavol.

\includegraphics[width=0.6\textwidth]{hw4_files/3f_lcavol_partreg.png}

Now, we take a look at the partial residual plot of the log of prostate weight, which actually shows an outlier. To the left is the partial residual plot with the outlier in, and to the right is the one with the outlier removed. Going back and digging through the data, this outlier is in fact case 32, which agrees with it being a high leverage point. The plot to the right definitely seems to show a better linear trend.

\noindent\includegraphics[width=0.5\textwidth]{hw4_files/3f_lweight.png}
\includegraphics[width=0.5\textwidth]{hw4_files/3f_lweight_noout.png}
\subsection*{4}
In order to have the dimensions match up, we must have $\beta_i\in\rn^{n_i}$. However, if the $n_i$ are distinct, we cannot compare the $\beta_i$ for equality. Thus, let's just suppose that all the $n_i$ are equal to $n$. Label the individual entries of the $X_i$ as $x_{ijk}$, where the $j$ and $k$ indicate the position of the entry within each $X_i$, and similarly with the $y_i\mapsto y_{ij}$. Then, under the null hypothesis, the model is $y_{ij}=\sum_k\beta_{k}x_{ijk}+e_{ij}$. Rewritten in matrix form, this is $y=X\beta+e$, where $y,e\in\rn^{rn}$, $X$ is the vertical concatenation of the $X_i$ (in $\rn^{rn\times p}$), and $\beta$ is the $p$-long coefficient vector. 

The alternative hypothesis is $y_{ij}=\sum_k\beta_{ik}x_{ijk}+e_{ij}$, or $y=X'\beta'+e$, where now $X$ is the block diagonal matrix formed by joining the $X_i$ along the diagonal and $\beta'$ is the vertical concatenation of the $\beta_i$. Thus, to test the null hypothesis, one could simply compare the smaller model obtained above to the larger alternative model using the standard $F$-test.
\subsection*{5}
We can compute the $F$-statistic. From 3.5 of Weisberg, the $F$-statistic for the test of the whole model versus just the intercept is $\frac{SS_{reg}/p}{RSS/(n-p)}$, and the $R^2$ value is $\frac{SS_{reg}}{SYY}$. Further, we have the relation $SYY=SS_{reg}+RSS$. Thus, we can rewrite $F=(n-p)\frac{SS_{reg}/p}{SYY-SS_{reg}}$, so $\frac{1}{F}=\frac{1}{n-p}\left(\frac{SYY}{SS_{reg}/p}-1\right)=\frac{1}{n-p}\left(\frac{p}{R^2}-1\right)$. Inverting this gives the $F$-statistic. Since $R^2\in(0,1)$, the probability of the inner term being zero is zero.
\subsection*{6}
From the slides, the $F$-statistic is the same as in the expression given, except the various norms are projections onto the image of the hat matrix for the different models instead. We just want to show that they're equal values. Let $\mathscr{H}$ be the small model, $\mathscr{L}$ be the big model, and $H_\mathscr{H}$ and $H_\mathscr{L}$ be the respective projection matrices. From the slides, we have that $\|\pi_{\mathscr{H}^\perp}(y)-\pi_{\mathscr{L}^\perp}(y)\|_2^2=\|\pi_\mathscr{L}(y)\|_2^2-\|\pi_\mathscr{H}(y)\|_2^2$ by the geometry of the configuration. Also, $\pi_\mathscr{L}$ is just $H_\mathscr{L}$, so this becomes $\|H_\mathscr{L}y\|_2^2-\|H_\mathscr{H}y\|_2^2$. Since we have $Hy=X\h{\beta}$, this is in fact just $\|X_2\h{\beta}_2\|_2^2-\|X_1\h{\beta}_2\|_2^2$, which is the same expression as in the numerator of the thing we were supposed to prove.

For the denominator, note that $\pi_{\mathscr{L}^\perp}$ is simply the residual vector of the larger model, so its norm squared is just equal to the RSS of the larger model, which is the same thing as in the expression we're trying to prove.
\end{document}
