\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\var}{var}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\h}[1]{\widehat{#1}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\nc{\vecx}{\mathbf{x}}
\nc{\vecy}{\mathbf{y}}
\nc{\vecv}{\mathbf{v}}
\begin{document}
Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{1}
\ssn{8.7.1}
We have $\h{U}=HU$. If $U=\openm1&0&\cdots&0\closem^T$, then $HU$ is just the first column of $H$, or $(h_{1j})$ for $1\leq j\leq n$.
\ssn{8.7.2}
The residuals are $U-\h{U}$. The first entry is $1-h_{11}$, and the other entries are $0-h_{1j}$ or $-h_{1j}$ for $2\leq j\leq n$.
\subsection*{2}
\ssn{a}
We have $\var(S_n)=\cov(S_n,S_n)$. Since covariance is a bilinear function, $\cov(\sum X_i, \sum X_i)=\sum\var(X_i)+\sum_{i\neq j}\cov(X_i,X_j)$. Since $\var(X_i)=\sigma^2$ for all $i$, this equals $n\sigma^2+\sigma^2\sum_{i\neq j}r_{ij}=n\sigma^2+n(n-1)\sigma^2r$.
\ssn{b}
Since $n$ is a scalar, $\var(S_n/n)=\frac{1}{n^2}\var(S_n)=\frac{\sigma^2}{n}+\frac{n-1}{n}\sigma^2r$
\ssn{c}
Ignore the $\sigma^2$ for the purposes of computing the ratio. The first term is $0.01$ when $n=100$, and the second term is $0.99\cdot r=0.0495$. This means that the contribution to the variance of $S_n$ from the codependence of the $X_i$ is almost $5$ times that of the contribution of the variance of the $X_i$, and so considering the $X_i$ independent based on $r<<1$ would result in a large mistake in the calculation of the variance of $S_n$.
\subsection*{3}

\begin{verbatim}
> summary(a)

Call:
lm(formula = gamble ~ sex + status + income + verbal, data = teengamb)

Residuals:
    Min      1Q  Median      3Q     Max 
-51.082 -11.320  -1.451   9.452  94.252 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept)  22.55565   17.19680   1.312   0.1968    
sex1        -22.11833    8.21111  -2.694   0.0101 *  
status        0.05223    0.28111   0.186   0.8535    
income        4.96198    1.02539   4.839 1.79e-05 ***
verbal       -2.95949    2.17215  -1.362   0.1803    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 22.69 on 42 degrees of freedom
Multiple R-squared:  0.5267,	Adjusted R-squared:  0.4816 
F-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06
\end{verbatim}
\ssn{a}
Percentage of variation explained is the $R^2$ ratio, which is $0.5267$ here.
\ssn{b}
A call to \verb|sort(residuals(a))| gives $94.252$ as the largest positive residual at case $24$.
\ssn{c}
Mean is $8.173\times10^{-16}$ and median is $-1.451$.
\ssn{d}
This can be done with \verb|cor(residuals(a), predict(a))|, which gives $2.13\times10^{-17}$, which is practically zero after accounting for machine precision.
\ssn{e}
\verb|cor(residuals(a),teengamb$income)| gives $2.97\times10^{-17}$.
\ssn{f}
This is simply the value of the $\beta$ parameter for sex, since the difference between males and females has magnitude $1$ when treated numerically. Depending on which gender got assigned to $1$ and which one got assigned to $0$, either the prediction is greater for males by $22.11$ than females or vice versa.
\subsection*{4}
Adapted from the proof found in Christensen.

Since $A$, $B$, and $V$ are all symmetric positive-semidefinite, we can apply the Cholesky decomposition to each of them. Let $A=L^TL$, $B=M^TM$, and $V=N^TN$. Then, $y^TAy=y^TL^TLy=(Ly)^T(Ly)$, and similarly $y^TBy=(My)^T(My)$. Now, if we can show that $Ly$ and $My$ are independent, we will have our result. This is the same as $\cov(Ly,My)=0$ (since $y$ is normal), or $L\cov(y,y)M^T=0$, or $LVM^T=0$. Using the decomposition of $V$, this is again equivalent to $LN^TNM^T=0$ or $(NL^T)^T(NM^T)=0$. This is the same as the image of $NL^T$ being orthogonal to that of $NM^T$, but since $\text{im}(AA^T)=\text{im}(A)$, this condition is again equivalent to $(NL^TLN^T)^T(NM^TMN^T)=0\iff NL^TLN^TNM^TMN^T=0\iff NAVBN^T=0$.

Now, examine the condition we're given, $VAVBV=0$. This is equivalent to $N^TNAVBV=0$, or $\text{im}(N^TN)\perp\text{im}(AVBV)$. Using the property of images on this, this is the same as $\text{im}(N^T)\perp\text{im}(AVBV)$, or $NAVBV=0$. Repeating the argument on the right side gives the desired result.

\subsection*{5}
Code is given in Python/Numpy/Scipy. 
\ssn{a}
\begin{verbatim}
X = np.matrix('10 9 9 11 11 10 10 12; 15 14 13 15 14 14 16 13').T
y = np.matrix('82 79 74 83 80 81 84 81').T

(q,r) = np.linalg.qr(X)

beta = np.linalg.solve(r, q.T * y)

print beta
\end{verbatim}
We have $\beta_1=2.648$ and $\beta_2=3.739$.
\begin{verbatim}
predicted = X * beta
residuals = y - predicted
rss = np.dot(residuals.T, residuals)
df = y.shape[0] - (X.shape[1])

sigmasq = rss / df
print sigmasq[0,0]
\end{verbatim}
This gives $\h{\sigma}^2=4.701$.
\ssn{b}
We have that $\frac{\h{\beta}_1-\beta_1}{\text{se}(\h{\beta}_1)}$ is distributed according to a $t_6$ distribution, and we can find $\text{se}(\h{\beta}_1)$ by looking at the appropriat element of $(X^TX)^{-1}\h{\sigma}^2$. Then,
\begin{verbatim}
beta_cov = linalg.inv(X.T * X) * sigmasq
beta1_se = np.sqrt(beta_cov[0,0])

tdist = scipy.stats.t(6)
t_crit_val = tdist.ppf(0.975)
print 'CI:({0}, {1})'.format(beta[0] - beta1_se*t_crit_val, beta[0] + beta1_se*t_crit_val)
\end{verbatim}
gives a confidence interval of $(1.121,4.174)$ for $\beta_1$.

As for $\beta_1+\beta_2$, let $R=\openm1&1\closem$. We have from the lecture notes that the set of points $R\beta$ where
\[(R\h{\beta}-R\beta)^T(R(X^TX)^{-1}R^T)^{-1}(R\h{\beta}-R\beta)<\h{\sigma}^2\cdot f\]
is the confidence interval for $\beta_1+\beta_2$ with $f$ as the critical value from the $F_{1,6}$ distribution at the $95\%$ level.

To compute this, we first obtain a value for $(R(X^TX)^{-1}R^T)^{-1}$ (henceforth named $s_1$) with the following code:
\begin{verbatim}
R = np.matrix("1 1")
xtxinv = linalg.inv(X.T * X)
s_1 = 1 / ((R * xtxinv * R.T)[0,0])
\end{verbatim}

Note that this is a scalar value. Now, since $R\h{\beta}$ and $R\beta$ are also scalars, we essentially have
\[
    (R\h{\beta}-R\beta)^2<\frac{\h{\sigma}^2\cdot f}{s_1}\\
    \implies R\h{\beta}-\h{\sigma}\sqrt{\frac{f}{s_1}}<R\beta<R\h{\beta}+\h{\sigma}\sqrt{\frac{f}{s_1}}
\]

Thus, we can finish computing the confidence interval with the following code:
\begin{verbatim}
rhb = beta.sum()
sigmahat = np.sqrt(sigmasq)
f_dist = scipy.stats.f(1,6)
f_crit = f_dist.ppf(0.95)
half_width = sigmahat * np.sqrt(f_crit / s_1)
print "CI for b_1+b_2: ({0}, {1})".format(rhb - half_width, rhb + half_width)
\end{verbatim}
finding a result of $(5.933,6.840)$.
\ssn{c}
The $t$-statistic for this test is $\frac{\h{\beta_2}-3}{\text{se}(\h{\beta_1}}$. Under the null hypothesis, this is distributed as $t_6$. We can compute the $p$-value as follows:
\begin{verbatim}
tval = (beta[1,0] - 3)/np.sqrt(beta_cov[1,1])
p_val = 1 - tdist.cdf(tval)
print tval, p_val
\end{verbatim}
which gives a $t$-value of $1.643$ and a $p$-value of $0.076$ (multiplied by $2$ since it's a two-sided test). Thus, we cannot reject the null hypothesis at the $\alpha=0.01$ level.
\ssn{d}
Using the distribution of $R\h{\beta}$ from the lecture notes again, if we set $R=\openm1&-1\closem$ and take the null hypothesis to be $R\beta=0$, then the $F$-statistic is
\[\frac{(R\h{\beta})^2(R(X^TX)^{-1}R^T)}{\h{\sigma}^2}\]
which follows a $F_{1,6}$ distribution under the null hypothesis. To obtain the $p$-value, we can use the following code:
\begin{verbatim}
R1 = np.matrix("1 -1")
s_2 = 1 / ((R1 * xtxinv * R1.T)[0,0])
rhb1 = (R1 * beta)[0,0]
f_stat = rhb1 * rhb1 * s_2 / sigmahat
print f_stat, 1 - f_dist.cdf(f_stat)
\end{verbatim}
This gives a $F$-statistic of $2.247$ and a corresponding $p$-value of $0.185$.
\end{document}
