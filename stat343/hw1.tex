\documentclass{article}
\usepackage{geometry}
\usepackage[namelimits,sumlimits]{amsmath}
\usepackage{amssymb,amsfonts}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[cm]{fullpage}
\newcommand{\tab}{\hspace*{5em}}
\newcommand{\conj}{\overline}
\newcommand{\dd}{\partial}
\newcommand{\ep}{\epsilon}
\newcommand{\openm}{\begin{pmatrix}}
\newcommand{\closem}{\end{pmatrix}}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\var}{var}
\newcommand{\nc}{\newcommand}
\newcommand{\rn}{\mathbb{R}}
\nc{\h}[1]{\widehat{#1}}
\nc{\ssn}[1]{\subsubsection*{#1}}
\begin{document}
Name: Hall Liu

Date: \today 
\vspace{1.5cm}

\subsection*{2.1}
\ssn{1}
\includegraphics[width=0.7\textwidth]{hw1_files/2_1_1.png}

Based on this plot, a simple linear regression model may not be the best choice -- there are two plausible ways to view the trend. Either we can treat the two points at $(165.3,77.8)$ and $(169.6,71.2)$ as outliers and fit the rest of the data to a linear model, or we can decide that the two points that look like outliers are actually the result of random fluctuation and include them in the fit as well.
\ssn{2}
Let the $x$ vector denote the heights, the $y$ vector denote the weights. We have $\sum x_i=1655.2$ and there are $10$ data points, so we have $\conj{x}=165.52$. Similarly, $\sum y_i=594.7$ so $\conj{y}=59.47$. I'm not really sure how to show my work for computing the other stuff, so here's some R code to compute S** (where * stands for either X or Y).
\begin{verbatim}
sss <- function(v1,v2) {
     v1_avg <- mean(v1); v2_avg <- mean(v2)
     sum((v1-v1_avg)*(v2-v2_avg))
}
\end{verbatim}

and so we obtain $SXX=472.076,SYY=731.961$, and $SXY=274.786$.
The slope is $SXY/SXX=0.582$, and the intercept is $\conj{y}-m\conj{x}=-36.87$. 

\includegraphics[width=0.7\textwidth]{hw1_files/2_1_2.png}
\ssn{3}
We have $\widehat{\sigma^2}=\frac{RSS}{8}$. Once again, here is a function to compute the RSS:
\begin{verbatim}
rss <- function(x, y, slope, intercept) {
    sum((y-(intercept+slope*x))^2)
}
\end{verbatim}
will calculate the RSS. Running this, we get $RSS=572.014$ so $\widehat{\sigma^2}=71.5$.

The estimate for the variance of $\h{\beta_1}$ is $\h{\sigma^2}/SXX=71.5/472.076=0.1514$ and the estimate for variance of $\h{\beta_0}$ is $\h{\sigma}^2\left(1/n+\conj{x}^2/SXX\right)=4156.64$. Estimated covariance is $\h{\sigma}^2\frac{\conj{x}}{SXX}=25.06$.

The $t$-statistic for $\h{\beta_0}$ is $\frac{\h{\beta_0}}{\sigma_{\beta_0}}=-0.572$ and the $t$-statistic for $\h{\beta_1}$ has the same form with a value of $1.496$. Both these values are distributed according to $t_8$ under the null hypothesis. Using a two-sided test, the $p$-value for $\h{\beta_0}$ is $0.583$ and the $p$-value for $\h{beta_1}$ is $0.173$.

\ssn{4}
The df of the regression is $1$ and the df of the residuals is $n-2$. The sum of squares for the regression is $SS_\text{res}=SYY-\frac{(SXY)^2}{SXX}=572.014$, and $SS_\text{reg}=\frac{(SXY)^2}{SXX}=159.947$. The mean of squares for each of the two is the $SS$ divided by df. The $F$-statistic is $\frac{MS_\text{reg}}{MS_\text{res}}=2.24$. Under the $F_{1,8}$ distribution, this has a $p$-value of $0.173$.

\begin{tabular}{cccccc}
\hline
Source & SS & df & MS & F & p\\
\hline
Regression & 159.947 & 1 & 159.947 & 2.24 & 0.173\\
Residuals & 572.014 & 8 & 71.50 \\
Total & 731.961& 9\\
\end{tabular}

Square root of $2.24$ is indeed $1.496$.

\subsection*{2.4}
\ssn{1}
R gives $\h{\beta_0}=29.917$ and $\h{\beta_1}=0.5417$, with $\se(\h{\beta_0})=1.622$ and $\se(\h{\beta{1}})=0.0.02596$. $r^2$ is $0.2408$, and the estimated variance of the residuals is $5.135$. The ANOVA table is below

\begin{tabular}{cccccc}
\hline
Source & SS & df & MS & F & p\\
\hline
Regression & 2237. & 1 & 2237. & 435.47 & Below precision\\
Residuals & 7052. & 1373 & 5.14 \\
Total &  9289. & 1374\\
\end{tabular}

In testing whether $\beta_1=0$, we find that the $p$-value is too low to be expressed effective by machine precision, so there's a very high chance that the heights of daughters depends on the heights of the mothers.
\ssn{2}
Write $E(Dheight|Mheight)=(\beta_0+\beta_1\cdot\conj{Mheight})+\beta_1(Mheight-\conj{Mheight})$. In this problem, the $\beta_1$ term states whether the deviation of the daughter's height from the mean tends to be larger than, the same as, or smaller than the deviation of the mother's height from the mean, depending on whether $\beta_1>1$, $\beta_1=1$, or $\beta_1<1$, respectively. 

As for the confidence interval for $\beta_1$, it's distributed according to $t_{1373}$, so the $t$-value for the $99.5$-th percentile is $2.5794$, resulting in a confidence interval of $(0.4748,0.6087)$.
\ssn{3}
The prediction is $64.589$ inches, with confidence interval $(58.74,70.44)$.
\subsection*{2.7}
\ssn{1}
Differentiating $RSS$ wrt $\beta_1$ and setting to $0$, we have $-2\sum(y_i-\beta_1x_i)x_i=0\implies \beta_1\sum x_i^2-\sum x_iy_i=0\implies \h{\beta_1}=\frac{\sum x_iy_i}{\sum x_i^2}$.

Calculating the expectation of $\h{\beta_1}$, we let $y_i=\beta_1x_i+e_i$, where $E(e_i)=0$, so $\h{\beta_1}=\frac{\sum \beta_1x_i^2+x_ie_i}{\sum x_i^2}$. Taking the expectation of this and using linearity, the $\sum x_i^2$ terms in the numerator and denominator cancel out to leave the $\beta_1$, and the remaining term with $\sum x_ie_i$ disappears due to the assumption on the residuals, so we have $E(\h{\beta_1})=\beta_1$.

Assume $\var(e_i)=\sigma^2$ for all $i$. To calculate variance, split the above expression for $\h{\beta_1}$ and observe that the first term is not random and therefore does not contribute to the variance. Hence, we want to calculate $\var\left(\frac{\sum x_ie_i}{\sum x_i^2}\right)$. Variance being linear, we can move it inside the sum and find that $\var(\h{\beta_1})=\frac{\sigma^2\sum x_i^2}{(\sum x_i^2)^2}=\frac{\sigma^2}{\sum x_i^2}$

Since the mean equation has only one parameter, the residuals have $n-1$ df, so the estimator for $\sigma^2$ is $\frac{RSS}{n-1}$.
\ssn{2}
Let the null hypothesis be that $\beta_0$ is zero. Then, the $SS_\text{reg}$ is the $RSS$ under the null hypothesis minus the $RSS$ under the alternative hypothesis, or $\sum(y_i-\h{\beta_1}x_i)^2-(S_{YY}-\frac{(S_{XY})^2}{S_{XX}}$. The $RSS$ of the null hypothesis has $n-1$ df and the $RSS$ of the alternative has $n-2$ df, so $SS_\text{reg}$ has $1$ df as before. 

\begin{tabular}{cccccc}
\hline
Source & SS & df & MS & F & p\\
\hline
Regression & $SS_\text{reg}$ & 1 & $SS_\text{reg}$ & $MS_\text{reg}/MS_\text{res}$ & \\
Residuals & $RSS$ & $n-1$ & $RSS/(n-2)$ \\
Total & $\sum (y_i-\h{\beta_1}x_i)^2$ & $n-1$\\
\end{tabular}

\ssn{3}
We have $\h{\beta_1}=0.5204$ and $\h{\sigma}^2=2.89$. A $95\%$ confidence interval for $\beta_1$ is $(0.492,0.548)$. The ANOVA table computed by R shows that the $F$-statistic is $0.2193$ leading to a $p$-value of $0.6463$, leading us to conclude that there is insufficient evidence to say that the intercept is nonzero.
\ssn{4}
\includegraphics[width=0.7\textwidth]{hw1_files/2_7_4.png}

Taking the intercept to be zero here seems to produce an acceptable result, as the residuals don't seem to follow any pattern about zero.
\subsection*{2.11}
Using the Ft.Collins snow data, R gives a $t$-statistic of $1.553$ for a $p$-value of $0.124$, and an $F$-statistic of $2.411$. Numerically, we have $t^2=F$ in this case.
\end{document}
